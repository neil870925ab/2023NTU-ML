{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n44rMWcLS-BY"
      },
      "source": [
        "# Week 10: Colab Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCMvaDkMTJkT"
      },
      "source": [
        "# I. Introduction\n",
        "In this exercise, we apply CNN to MNIST data to classify the hand written digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jxq00nbuCwt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSfKVVBbRLbh"
      },
      "source": [
        "# Data Loading\n",
        "Load the data from the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoUAesyDuL0n"
      },
      "outputs": [],
      "source": [
        "# Run this once to load the train and test data straight into a dataloader class\n",
        "# that will provide the batches\n",
        "batch_size_train = 64\n",
        "batch_size_test = 1000\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_test, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVFa1TlARLbh"
      },
      "source": [
        "# Visualize dataset sample\n",
        "Show some sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "boEAxlB5uPZx",
        "outputId": "2712e6a8-b352-4f20-e5b4-25998a6bf804"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 6 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "# Let's draw some of the training data\n",
        "examples = enumerate(test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_94InEd1TkC6"
      },
      "source": [
        "# II. Methods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_kwOzXQuWNK"
      },
      "outputs": [],
      "source": [
        "from os import X_OK\n",
        "\n",
        "# This class implements a minimal network (which still does okay)\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Valid convolution, 1 channel in, 2 channels out, stride 1, kernel size = 3\n",
        "        self.conv1 = nn.Conv2d(1, 2, kernel_size=3)\n",
        "        # Dropout for convolutions\n",
        "        self.drop = nn.Dropout2d()\n",
        "        # Fully connected layer\n",
        "        self.fc1 = nn.Linear(338, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.drop(x)\n",
        "        x = F.max_pool2d(x,2)\n",
        "        x = F.relu(x)\n",
        "        x = x.flatten(1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.log_softmax(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4Ue45Pnf8gZ"
      },
      "outputs": [],
      "source": [
        "# TODO Change above Net to Net2 class to implement\n",
        "# 1. A valid convolution with kernel size 5, 1 input channel and 10 output channels\n",
        "# 2. A max pooling operation over a 2x2 area\n",
        "# 3. A Relu\n",
        "# 4. A valid convolution with kernel size 5, 10 input channels and 20 output channels\n",
        "# 5. A 2D Dropout layer\n",
        "# 6. A max pooling operation over a 2x2 area\n",
        "# 7. A relu\n",
        "# 8. A flattening operation\n",
        "# 9. A fully connected layer mapping from (whatever dimensions we are at-- find out using .shape) to 50\n",
        "# 10. A ReLU\n",
        "# 11. A fully connected layer mapping from 50 to 10 dimensions\n",
        "# 12. A softmax function.\n",
        "\n",
        "class Net2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net2, self).__init__()\n",
        "        # Valid convolution, 1 channel in, 10 channels out, kernel size = 5\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        # Valid convolution, 10 channel in, 20 channels out, kernel size = 5\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(20 * 4 * 4, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.max_pool2d(x,2)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.drop(x)\n",
        "        x = F.max_pool2d(x,2)\n",
        "        x = F.relu(x)\n",
        "        x = x.flatten(1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.log_softmax(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sN5hsK2uan8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# He initialization of weights\n",
        "def weights_init(layer_in):\n",
        "  if isinstance(layer_in, nn.Linear):\n",
        "    nn.init.kaiming_uniform_(layer_in.weight)\n",
        "    layer_in.bias.data.fill_(0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pBDgYp2ufUi"
      },
      "outputs": [],
      "source": [
        "# Main training routine\n",
        "# TODO: Read it and understand what it does, you would need to implement it in the next colab HW\n",
        "def train(epoch, model):\n",
        "  model.train()\n",
        "  # Get each\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # Store results\n",
        "    if batch_idx % 10 == 0:\n",
        "\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct = pred.eq(target.data.view_as(pred)).sum()\n",
        "      print('Train Epoch: {} [{}/{}]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset), loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xr6yXzWduhbU"
      },
      "outputs": [],
      "source": [
        "# Run on test data\n",
        "# TODO: Read it and understand what it does, you would need to implement it in the next colab HW\n",
        "def test(model):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      output = model(data)\n",
        "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))\n",
        "  return 100. * correct / len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVUrbYiamki8",
        "outputId": "691872ad-d593-4906-8b74-ae657af582be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-b8ebf8cbff0b>:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Avg. loss: 2.9730, Accuracy: 882/10000 (9%)\n",
            "\n",
            "Train Epoch: 1 [0/60000]\tLoss: 3.377559\n",
            "Train Epoch: 1 [640/60000]\tLoss: 2.328599\n",
            "Train Epoch: 1 [1280/60000]\tLoss: 2.331368\n",
            "Train Epoch: 1 [1920/60000]\tLoss: 2.294677\n",
            "Train Epoch: 1 [2560/60000]\tLoss: 2.163913\n",
            "Train Epoch: 1 [3200/60000]\tLoss: 2.213560\n",
            "Train Epoch: 1 [3840/60000]\tLoss: 2.084686\n",
            "Train Epoch: 1 [4480/60000]\tLoss: 2.019122\n",
            "Train Epoch: 1 [5120/60000]\tLoss: 1.882441\n",
            "Train Epoch: 1 [5760/60000]\tLoss: 1.923202\n",
            "Train Epoch: 1 [6400/60000]\tLoss: 1.698973\n",
            "Train Epoch: 1 [7040/60000]\tLoss: 1.386831\n",
            "Train Epoch: 1 [7680/60000]\tLoss: 1.632334\n",
            "Train Epoch: 1 [8320/60000]\tLoss: 1.436194\n",
            "Train Epoch: 1 [8960/60000]\tLoss: 1.193733\n",
            "Train Epoch: 1 [9600/60000]\tLoss: 1.280024\n",
            "Train Epoch: 1 [10240/60000]\tLoss: 1.285253\n",
            "Train Epoch: 1 [10880/60000]\tLoss: 1.069128\n",
            "Train Epoch: 1 [11520/60000]\tLoss: 1.009951\n",
            "Train Epoch: 1 [12160/60000]\tLoss: 1.163940\n",
            "Train Epoch: 1 [12800/60000]\tLoss: 1.521019\n",
            "Train Epoch: 1 [13440/60000]\tLoss: 1.213986\n",
            "Train Epoch: 1 [14080/60000]\tLoss: 1.330893\n",
            "Train Epoch: 1 [14720/60000]\tLoss: 1.221216\n",
            "Train Epoch: 1 [15360/60000]\tLoss: 1.108900\n",
            "Train Epoch: 1 [16000/60000]\tLoss: 0.928387\n",
            "Train Epoch: 1 [16640/60000]\tLoss: 1.170225\n",
            "Train Epoch: 1 [17280/60000]\tLoss: 1.229838\n",
            "Train Epoch: 1 [17920/60000]\tLoss: 0.989563\n",
            "Train Epoch: 1 [18560/60000]\tLoss: 1.246194\n",
            "Train Epoch: 1 [19200/60000]\tLoss: 0.954874\n",
            "Train Epoch: 1 [19840/60000]\tLoss: 1.013148\n",
            "Train Epoch: 1 [20480/60000]\tLoss: 1.104350\n",
            "Train Epoch: 1 [21120/60000]\tLoss: 1.221387\n",
            "Train Epoch: 1 [21760/60000]\tLoss: 0.884082\n",
            "Train Epoch: 1 [22400/60000]\tLoss: 1.161757\n",
            "Train Epoch: 1 [23040/60000]\tLoss: 1.228086\n",
            "Train Epoch: 1 [23680/60000]\tLoss: 1.056476\n",
            "Train Epoch: 1 [24320/60000]\tLoss: 0.839773\n",
            "Train Epoch: 1 [24960/60000]\tLoss: 0.644582\n",
            "Train Epoch: 1 [25600/60000]\tLoss: 1.076161\n",
            "Train Epoch: 1 [26240/60000]\tLoss: 0.913450\n",
            "Train Epoch: 1 [26880/60000]\tLoss: 1.153267\n",
            "Train Epoch: 1 [27520/60000]\tLoss: 0.904205\n",
            "Train Epoch: 1 [28160/60000]\tLoss: 1.333728\n",
            "Train Epoch: 1 [28800/60000]\tLoss: 0.797534\n",
            "Train Epoch: 1 [29440/60000]\tLoss: 0.833751\n",
            "Train Epoch: 1 [30080/60000]\tLoss: 0.913830\n",
            "Train Epoch: 1 [30720/60000]\tLoss: 0.943100\n",
            "Train Epoch: 1 [31360/60000]\tLoss: 0.832472\n",
            "Train Epoch: 1 [32000/60000]\tLoss: 1.154091\n",
            "Train Epoch: 1 [32640/60000]\tLoss: 0.919517\n",
            "Train Epoch: 1 [33280/60000]\tLoss: 0.787422\n",
            "Train Epoch: 1 [33920/60000]\tLoss: 1.097351\n",
            "Train Epoch: 1 [34560/60000]\tLoss: 0.732477\n",
            "Train Epoch: 1 [35200/60000]\tLoss: 0.874950\n",
            "Train Epoch: 1 [35840/60000]\tLoss: 0.583291\n",
            "Train Epoch: 1 [36480/60000]\tLoss: 0.890994\n",
            "Train Epoch: 1 [37120/60000]\tLoss: 0.905803\n",
            "Train Epoch: 1 [37760/60000]\tLoss: 1.030741\n",
            "Train Epoch: 1 [38400/60000]\tLoss: 1.023605\n",
            "Train Epoch: 1 [39040/60000]\tLoss: 1.314835\n",
            "Train Epoch: 1 [39680/60000]\tLoss: 1.033340\n",
            "Train Epoch: 1 [40320/60000]\tLoss: 0.931946\n",
            "Train Epoch: 1 [40960/60000]\tLoss: 0.955480\n",
            "Train Epoch: 1 [41600/60000]\tLoss: 1.141148\n",
            "Train Epoch: 1 [42240/60000]\tLoss: 0.885406\n",
            "Train Epoch: 1 [42880/60000]\tLoss: 0.967648\n",
            "Train Epoch: 1 [43520/60000]\tLoss: 0.829934\n",
            "Train Epoch: 1 [44160/60000]\tLoss: 0.808725\n",
            "Train Epoch: 1 [44800/60000]\tLoss: 1.108800\n",
            "Train Epoch: 1 [45440/60000]\tLoss: 0.615649\n",
            "Train Epoch: 1 [46080/60000]\tLoss: 0.712712\n",
            "Train Epoch: 1 [46720/60000]\tLoss: 0.768054\n",
            "Train Epoch: 1 [47360/60000]\tLoss: 0.872109\n",
            "Train Epoch: 1 [48000/60000]\tLoss: 0.728669\n",
            "Train Epoch: 1 [48640/60000]\tLoss: 0.869304\n",
            "Train Epoch: 1 [49280/60000]\tLoss: 0.927112\n",
            "Train Epoch: 1 [49920/60000]\tLoss: 0.908872\n",
            "Train Epoch: 1 [50560/60000]\tLoss: 0.915916\n",
            "Train Epoch: 1 [51200/60000]\tLoss: 0.760278\n",
            "Train Epoch: 1 [51840/60000]\tLoss: 0.765670\n",
            "Train Epoch: 1 [52480/60000]\tLoss: 1.026265\n",
            "Train Epoch: 1 [53120/60000]\tLoss: 0.883088\n",
            "Train Epoch: 1 [53760/60000]\tLoss: 1.132757\n",
            "Train Epoch: 1 [54400/60000]\tLoss: 0.903521\n",
            "Train Epoch: 1 [55040/60000]\tLoss: 0.933078\n",
            "Train Epoch: 1 [55680/60000]\tLoss: 0.757861\n",
            "Train Epoch: 1 [56320/60000]\tLoss: 0.880782\n",
            "Train Epoch: 1 [56960/60000]\tLoss: 0.949869\n",
            "Train Epoch: 1 [57600/60000]\tLoss: 1.017176\n",
            "Train Epoch: 1 [58240/60000]\tLoss: 0.781143\n",
            "Train Epoch: 1 [58880/60000]\tLoss: 0.978289\n",
            "Train Epoch: 1 [59520/60000]\tLoss: 0.680523\n",
            "Train Epoch: 2 [0/60000]\tLoss: 0.957399\n",
            "Train Epoch: 2 [640/60000]\tLoss: 0.956641\n",
            "Train Epoch: 2 [1280/60000]\tLoss: 0.837796\n",
            "Train Epoch: 2 [1920/60000]\tLoss: 0.837765\n",
            "Train Epoch: 2 [2560/60000]\tLoss: 0.755709\n",
            "Train Epoch: 2 [3200/60000]\tLoss: 0.654398\n",
            "Train Epoch: 2 [3840/60000]\tLoss: 0.780738\n",
            "Train Epoch: 2 [4480/60000]\tLoss: 0.984900\n",
            "Train Epoch: 2 [5120/60000]\tLoss: 0.750959\n",
            "Train Epoch: 2 [5760/60000]\tLoss: 0.759252\n",
            "Train Epoch: 2 [6400/60000]\tLoss: 0.861907\n",
            "Train Epoch: 2 [7040/60000]\tLoss: 0.936828\n",
            "Train Epoch: 2 [7680/60000]\tLoss: 0.708157\n",
            "Train Epoch: 2 [8320/60000]\tLoss: 0.858937\n",
            "Train Epoch: 2 [8960/60000]\tLoss: 1.005341\n",
            "Train Epoch: 2 [9600/60000]\tLoss: 1.088898\n",
            "Train Epoch: 2 [10240/60000]\tLoss: 0.741673\n",
            "Train Epoch: 2 [10880/60000]\tLoss: 0.887322\n",
            "Train Epoch: 2 [11520/60000]\tLoss: 1.011826\n",
            "Train Epoch: 2 [12160/60000]\tLoss: 0.939573\n",
            "Train Epoch: 2 [12800/60000]\tLoss: 0.915086\n",
            "Train Epoch: 2 [13440/60000]\tLoss: 0.926684\n",
            "Train Epoch: 2 [14080/60000]\tLoss: 1.064332\n",
            "Train Epoch: 2 [14720/60000]\tLoss: 0.750861\n",
            "Train Epoch: 2 [15360/60000]\tLoss: 0.814014\n",
            "Train Epoch: 2 [16000/60000]\tLoss: 0.995495\n",
            "Train Epoch: 2 [16640/60000]\tLoss: 0.976024\n",
            "Train Epoch: 2 [17280/60000]\tLoss: 0.808426\n",
            "Train Epoch: 2 [17920/60000]\tLoss: 1.047388\n",
            "Train Epoch: 2 [18560/60000]\tLoss: 0.836760\n",
            "Train Epoch: 2 [19200/60000]\tLoss: 1.257390\n",
            "Train Epoch: 2 [19840/60000]\tLoss: 0.887555\n",
            "Train Epoch: 2 [20480/60000]\tLoss: 0.644889\n",
            "Train Epoch: 2 [21120/60000]\tLoss: 0.917053\n",
            "Train Epoch: 2 [21760/60000]\tLoss: 0.939393\n",
            "Train Epoch: 2 [22400/60000]\tLoss: 0.661200\n",
            "Train Epoch: 2 [23040/60000]\tLoss: 0.779952\n",
            "Train Epoch: 2 [23680/60000]\tLoss: 0.667818\n",
            "Train Epoch: 2 [24320/60000]\tLoss: 1.011640\n",
            "Train Epoch: 2 [24960/60000]\tLoss: 0.873563\n",
            "Train Epoch: 2 [25600/60000]\tLoss: 0.975328\n",
            "Train Epoch: 2 [26240/60000]\tLoss: 0.742849\n",
            "Train Epoch: 2 [26880/60000]\tLoss: 0.754777\n",
            "Train Epoch: 2 [27520/60000]\tLoss: 1.081446\n",
            "Train Epoch: 2 [28160/60000]\tLoss: 0.796717\n",
            "Train Epoch: 2 [28800/60000]\tLoss: 0.921516\n",
            "Train Epoch: 2 [29440/60000]\tLoss: 0.963455\n",
            "Train Epoch: 2 [30080/60000]\tLoss: 0.583721\n",
            "Train Epoch: 2 [30720/60000]\tLoss: 0.970650\n",
            "Train Epoch: 2 [31360/60000]\tLoss: 0.849905\n",
            "Train Epoch: 2 [32000/60000]\tLoss: 0.878065\n",
            "Train Epoch: 2 [32640/60000]\tLoss: 0.957238\n",
            "Train Epoch: 2 [33280/60000]\tLoss: 0.894545\n",
            "Train Epoch: 2 [33920/60000]\tLoss: 0.720663\n",
            "Train Epoch: 2 [34560/60000]\tLoss: 0.877283\n",
            "Train Epoch: 2 [35200/60000]\tLoss: 0.552074\n",
            "Train Epoch: 2 [35840/60000]\tLoss: 0.864437\n",
            "Train Epoch: 2 [36480/60000]\tLoss: 0.885974\n",
            "Train Epoch: 2 [37120/60000]\tLoss: 0.831351\n",
            "Train Epoch: 2 [37760/60000]\tLoss: 0.717593\n",
            "Train Epoch: 2 [38400/60000]\tLoss: 0.992868\n",
            "Train Epoch: 2 [39040/60000]\tLoss: 1.201016\n",
            "Train Epoch: 2 [39680/60000]\tLoss: 0.803585\n",
            "Train Epoch: 2 [40320/60000]\tLoss: 0.826697\n",
            "Train Epoch: 2 [40960/60000]\tLoss: 0.761090\n",
            "Train Epoch: 2 [41600/60000]\tLoss: 0.734727\n",
            "Train Epoch: 2 [42240/60000]\tLoss: 1.134038\n",
            "Train Epoch: 2 [42880/60000]\tLoss: 0.930942\n",
            "Train Epoch: 2 [43520/60000]\tLoss: 0.630517\n",
            "Train Epoch: 2 [44160/60000]\tLoss: 0.881334\n",
            "Train Epoch: 2 [44800/60000]\tLoss: 0.932641\n",
            "Train Epoch: 2 [45440/60000]\tLoss: 0.948288\n",
            "Train Epoch: 2 [46080/60000]\tLoss: 0.751079\n",
            "Train Epoch: 2 [46720/60000]\tLoss: 0.910188\n",
            "Train Epoch: 2 [47360/60000]\tLoss: 1.092092\n",
            "Train Epoch: 2 [48000/60000]\tLoss: 0.793293\n",
            "Train Epoch: 2 [48640/60000]\tLoss: 0.761581\n",
            "Train Epoch: 2 [49280/60000]\tLoss: 0.673720\n",
            "Train Epoch: 2 [49920/60000]\tLoss: 0.806540\n",
            "Train Epoch: 2 [50560/60000]\tLoss: 0.856548\n",
            "Train Epoch: 2 [51200/60000]\tLoss: 0.847610\n",
            "Train Epoch: 2 [51840/60000]\tLoss: 0.667706\n",
            "Train Epoch: 2 [52480/60000]\tLoss: 0.876347\n",
            "Train Epoch: 2 [53120/60000]\tLoss: 0.920745\n",
            "Train Epoch: 2 [53760/60000]\tLoss: 1.244776\n",
            "Train Epoch: 2 [54400/60000]\tLoss: 0.789147\n",
            "Train Epoch: 2 [55040/60000]\tLoss: 0.816818\n",
            "Train Epoch: 2 [55680/60000]\tLoss: 0.718161\n",
            "Train Epoch: 2 [56320/60000]\tLoss: 0.872373\n",
            "Train Epoch: 2 [56960/60000]\tLoss: 0.632330\n",
            "Train Epoch: 2 [57600/60000]\tLoss: 0.913124\n",
            "Train Epoch: 2 [58240/60000]\tLoss: 0.943950\n",
            "Train Epoch: 2 [58880/60000]\tLoss: 0.665456\n",
            "Train Epoch: 2 [59520/60000]\tLoss: 1.051373\n",
            "Train Epoch: 3 [0/60000]\tLoss: 0.952335\n",
            "Train Epoch: 3 [640/60000]\tLoss: 0.909226\n",
            "Train Epoch: 3 [1280/60000]\tLoss: 0.833932\n",
            "Train Epoch: 3 [1920/60000]\tLoss: 0.893859\n",
            "Train Epoch: 3 [2560/60000]\tLoss: 0.833597\n",
            "Train Epoch: 3 [3200/60000]\tLoss: 0.845560\n",
            "Train Epoch: 3 [3840/60000]\tLoss: 1.123060\n",
            "Train Epoch: 3 [4480/60000]\tLoss: 0.635893\n",
            "Train Epoch: 3 [5120/60000]\tLoss: 0.754516\n",
            "Train Epoch: 3 [5760/60000]\tLoss: 0.850427\n",
            "Train Epoch: 3 [6400/60000]\tLoss: 0.942886\n",
            "Train Epoch: 3 [7040/60000]\tLoss: 1.022393\n",
            "Train Epoch: 3 [7680/60000]\tLoss: 0.838696\n",
            "Train Epoch: 3 [8320/60000]\tLoss: 0.952766\n",
            "Train Epoch: 3 [8960/60000]\tLoss: 0.811089\n",
            "Train Epoch: 3 [9600/60000]\tLoss: 0.739863\n",
            "Train Epoch: 3 [10240/60000]\tLoss: 1.003845\n",
            "Train Epoch: 3 [10880/60000]\tLoss: 0.919795\n",
            "Train Epoch: 3 [11520/60000]\tLoss: 0.700008\n",
            "Train Epoch: 3 [12160/60000]\tLoss: 1.053547\n",
            "Train Epoch: 3 [12800/60000]\tLoss: 0.752537\n",
            "Train Epoch: 3 [13440/60000]\tLoss: 0.728470\n",
            "Train Epoch: 3 [14080/60000]\tLoss: 1.061936\n",
            "Train Epoch: 3 [14720/60000]\tLoss: 0.908319\n",
            "Train Epoch: 3 [15360/60000]\tLoss: 0.792712\n",
            "Train Epoch: 3 [16000/60000]\tLoss: 0.847512\n",
            "Train Epoch: 3 [16640/60000]\tLoss: 0.772565\n",
            "Train Epoch: 3 [17280/60000]\tLoss: 0.701768\n",
            "Train Epoch: 3 [17920/60000]\tLoss: 0.656770\n",
            "Train Epoch: 3 [18560/60000]\tLoss: 0.848669\n",
            "Train Epoch: 3 [19200/60000]\tLoss: 0.724682\n",
            "Train Epoch: 3 [19840/60000]\tLoss: 1.041067\n",
            "Train Epoch: 3 [20480/60000]\tLoss: 0.782779\n",
            "Train Epoch: 3 [21120/60000]\tLoss: 0.717369\n",
            "Train Epoch: 3 [21760/60000]\tLoss: 0.816657\n",
            "Train Epoch: 3 [22400/60000]\tLoss: 0.824746\n",
            "Train Epoch: 3 [23040/60000]\tLoss: 0.807206\n",
            "Train Epoch: 3 [23680/60000]\tLoss: 0.968741\n",
            "Train Epoch: 3 [24320/60000]\tLoss: 0.721772\n",
            "Train Epoch: 3 [24960/60000]\tLoss: 0.829547\n",
            "Train Epoch: 3 [25600/60000]\tLoss: 0.763872\n",
            "Train Epoch: 3 [26240/60000]\tLoss: 0.763281\n",
            "Train Epoch: 3 [26880/60000]\tLoss: 0.611882\n",
            "Train Epoch: 3 [27520/60000]\tLoss: 0.793862\n",
            "Train Epoch: 3 [28160/60000]\tLoss: 1.008635\n",
            "Train Epoch: 3 [28800/60000]\tLoss: 0.667138\n",
            "Train Epoch: 3 [29440/60000]\tLoss: 0.942522\n",
            "Train Epoch: 3 [30080/60000]\tLoss: 0.778701\n",
            "Train Epoch: 3 [30720/60000]\tLoss: 0.741112\n",
            "Train Epoch: 3 [31360/60000]\tLoss: 0.896639\n",
            "Train Epoch: 3 [32000/60000]\tLoss: 0.903613\n",
            "Train Epoch: 3 [32640/60000]\tLoss: 0.763083\n",
            "Train Epoch: 3 [33280/60000]\tLoss: 0.714331\n",
            "Train Epoch: 3 [33920/60000]\tLoss: 0.764561\n",
            "Train Epoch: 3 [34560/60000]\tLoss: 0.742465\n",
            "Train Epoch: 3 [35200/60000]\tLoss: 0.818717\n",
            "Train Epoch: 3 [35840/60000]\tLoss: 0.777154\n",
            "Train Epoch: 3 [36480/60000]\tLoss: 0.868144\n",
            "Train Epoch: 3 [37120/60000]\tLoss: 0.593096\n",
            "Train Epoch: 3 [37760/60000]\tLoss: 1.031028\n",
            "Train Epoch: 3 [38400/60000]\tLoss: 0.703864\n",
            "Train Epoch: 3 [39040/60000]\tLoss: 0.609352\n",
            "Train Epoch: 3 [39680/60000]\tLoss: 0.972125\n",
            "Train Epoch: 3 [40320/60000]\tLoss: 0.619043\n",
            "Train Epoch: 3 [40960/60000]\tLoss: 0.771257\n",
            "Train Epoch: 3 [41600/60000]\tLoss: 0.581550\n",
            "Train Epoch: 3 [42240/60000]\tLoss: 0.753618\n",
            "Train Epoch: 3 [42880/60000]\tLoss: 0.776185\n",
            "Train Epoch: 3 [43520/60000]\tLoss: 0.679750\n",
            "Train Epoch: 3 [44160/60000]\tLoss: 0.745104\n",
            "Train Epoch: 3 [44800/60000]\tLoss: 0.502415\n",
            "Train Epoch: 3 [45440/60000]\tLoss: 0.826418\n",
            "Train Epoch: 3 [46080/60000]\tLoss: 0.923066\n",
            "Train Epoch: 3 [46720/60000]\tLoss: 0.940601\n",
            "Train Epoch: 3 [47360/60000]\tLoss: 0.587821\n",
            "Train Epoch: 3 [48000/60000]\tLoss: 0.603018\n",
            "Train Epoch: 3 [48640/60000]\tLoss: 0.898089\n",
            "Train Epoch: 3 [49280/60000]\tLoss: 0.869170\n",
            "Train Epoch: 3 [49920/60000]\tLoss: 0.897727\n",
            "Train Epoch: 3 [50560/60000]\tLoss: 0.723732\n",
            "Train Epoch: 3 [51200/60000]\tLoss: 0.869734\n",
            "Train Epoch: 3 [51840/60000]\tLoss: 0.890630\n",
            "Train Epoch: 3 [52480/60000]\tLoss: 0.910684\n",
            "Train Epoch: 3 [53120/60000]\tLoss: 0.771762\n",
            "Train Epoch: 3 [53760/60000]\tLoss: 0.558848\n",
            "Train Epoch: 3 [54400/60000]\tLoss: 1.257754\n",
            "Train Epoch: 3 [55040/60000]\tLoss: 0.944150\n",
            "Train Epoch: 3 [55680/60000]\tLoss: 0.748081\n",
            "Train Epoch: 3 [56320/60000]\tLoss: 0.853649\n",
            "Train Epoch: 3 [56960/60000]\tLoss: 0.898974\n",
            "Train Epoch: 3 [57600/60000]\tLoss: 0.693905\n",
            "Train Epoch: 3 [58240/60000]\tLoss: 0.631951\n",
            "Train Epoch: 3 [58880/60000]\tLoss: 0.914406\n",
            "Train Epoch: 3 [59520/60000]\tLoss: 1.141128\n",
            "Train Epoch: 4 [0/60000]\tLoss: 1.025316\n",
            "Train Epoch: 4 [640/60000]\tLoss: 0.741359\n",
            "Train Epoch: 4 [1280/60000]\tLoss: 0.973549\n",
            "Train Epoch: 4 [1920/60000]\tLoss: 0.712551\n",
            "Train Epoch: 4 [2560/60000]\tLoss: 0.940713\n",
            "Train Epoch: 4 [3200/60000]\tLoss: 0.952869\n",
            "Train Epoch: 4 [3840/60000]\tLoss: 0.816272\n",
            "Train Epoch: 4 [4480/60000]\tLoss: 0.651407\n",
            "Train Epoch: 4 [5120/60000]\tLoss: 0.777243\n",
            "Train Epoch: 4 [5760/60000]\tLoss: 0.940348\n",
            "Train Epoch: 4 [6400/60000]\tLoss: 0.722787\n",
            "Train Epoch: 4 [7040/60000]\tLoss: 0.760806\n",
            "Train Epoch: 4 [7680/60000]\tLoss: 1.010417\n",
            "Train Epoch: 4 [8320/60000]\tLoss: 0.886166\n",
            "Train Epoch: 4 [8960/60000]\tLoss: 0.758735\n",
            "Train Epoch: 4 [9600/60000]\tLoss: 0.992544\n",
            "Train Epoch: 4 [10240/60000]\tLoss: 0.805936\n",
            "Train Epoch: 4 [10880/60000]\tLoss: 0.880110\n",
            "Train Epoch: 4 [11520/60000]\tLoss: 0.733765\n",
            "Train Epoch: 4 [12160/60000]\tLoss: 0.763912\n",
            "Train Epoch: 4 [12800/60000]\tLoss: 0.807716\n",
            "Train Epoch: 4 [13440/60000]\tLoss: 0.951271\n",
            "Train Epoch: 4 [14080/60000]\tLoss: 0.718394\n",
            "Train Epoch: 4 [14720/60000]\tLoss: 1.083996\n",
            "Train Epoch: 4 [15360/60000]\tLoss: 0.965343\n",
            "Train Epoch: 4 [16000/60000]\tLoss: 0.858347\n",
            "Train Epoch: 4 [16640/60000]\tLoss: 0.763800\n",
            "Train Epoch: 4 [17280/60000]\tLoss: 0.803037\n",
            "Train Epoch: 4 [17920/60000]\tLoss: 1.150355\n",
            "Train Epoch: 4 [18560/60000]\tLoss: 0.617443\n",
            "Train Epoch: 4 [19200/60000]\tLoss: 0.690953\n",
            "Train Epoch: 4 [19840/60000]\tLoss: 1.016359\n",
            "Train Epoch: 4 [20480/60000]\tLoss: 0.642402\n",
            "Train Epoch: 4 [21120/60000]\tLoss: 0.718967\n",
            "Train Epoch: 4 [21760/60000]\tLoss: 0.801007\n",
            "Train Epoch: 4 [22400/60000]\tLoss: 0.719810\n",
            "Train Epoch: 4 [23040/60000]\tLoss: 0.935244\n",
            "Train Epoch: 4 [23680/60000]\tLoss: 0.894713\n",
            "Train Epoch: 4 [24320/60000]\tLoss: 0.831324\n",
            "Train Epoch: 4 [24960/60000]\tLoss: 0.495929\n",
            "Train Epoch: 4 [25600/60000]\tLoss: 0.955822\n",
            "Train Epoch: 4 [26240/60000]\tLoss: 0.809001\n",
            "Train Epoch: 4 [26880/60000]\tLoss: 0.813654\n",
            "Train Epoch: 4 [27520/60000]\tLoss: 0.836861\n",
            "Train Epoch: 4 [28160/60000]\tLoss: 1.058636\n",
            "Train Epoch: 4 [28800/60000]\tLoss: 0.785297\n",
            "Train Epoch: 4 [29440/60000]\tLoss: 0.993833\n",
            "Train Epoch: 4 [30080/60000]\tLoss: 0.754647\n",
            "Train Epoch: 4 [30720/60000]\tLoss: 1.113268\n",
            "Train Epoch: 4 [31360/60000]\tLoss: 0.975120\n",
            "Train Epoch: 4 [32000/60000]\tLoss: 0.439252\n",
            "Train Epoch: 4 [32640/60000]\tLoss: 0.753537\n",
            "Train Epoch: 4 [33280/60000]\tLoss: 0.737405\n",
            "Train Epoch: 4 [33920/60000]\tLoss: 0.838479\n",
            "Train Epoch: 4 [34560/60000]\tLoss: 0.758767\n",
            "Train Epoch: 4 [35200/60000]\tLoss: 0.872914\n",
            "Train Epoch: 4 [35840/60000]\tLoss: 0.757807\n",
            "Train Epoch: 4 [36480/60000]\tLoss: 0.975747\n",
            "Train Epoch: 4 [37120/60000]\tLoss: 0.801395\n",
            "Train Epoch: 4 [37760/60000]\tLoss: 1.087991\n",
            "Train Epoch: 4 [38400/60000]\tLoss: 0.796167\n",
            "Train Epoch: 4 [39040/60000]\tLoss: 0.752333\n",
            "Train Epoch: 4 [39680/60000]\tLoss: 1.008781\n",
            "Train Epoch: 4 [40320/60000]\tLoss: 0.967899\n",
            "Train Epoch: 4 [40960/60000]\tLoss: 0.566111\n",
            "Train Epoch: 4 [41600/60000]\tLoss: 1.202060\n",
            "Train Epoch: 4 [42240/60000]\tLoss: 0.708861\n",
            "Train Epoch: 4 [42880/60000]\tLoss: 0.989594\n",
            "Train Epoch: 4 [43520/60000]\tLoss: 0.992162\n",
            "Train Epoch: 4 [44160/60000]\tLoss: 0.848192\n",
            "Train Epoch: 4 [44800/60000]\tLoss: 0.852514\n",
            "Train Epoch: 4 [45440/60000]\tLoss: 0.713178\n",
            "Train Epoch: 4 [46080/60000]\tLoss: 0.843986\n",
            "Train Epoch: 4 [46720/60000]\tLoss: 0.643850\n",
            "Train Epoch: 4 [47360/60000]\tLoss: 1.053935\n",
            "Train Epoch: 4 [48000/60000]\tLoss: 0.902796\n",
            "Train Epoch: 4 [48640/60000]\tLoss: 0.570937\n",
            "Train Epoch: 4 [49280/60000]\tLoss: 0.673955\n",
            "Train Epoch: 4 [49920/60000]\tLoss: 0.569550\n",
            "Train Epoch: 4 [50560/60000]\tLoss: 0.785687\n",
            "Train Epoch: 4 [51200/60000]\tLoss: 0.912522\n",
            "Train Epoch: 4 [51840/60000]\tLoss: 0.663035\n",
            "Train Epoch: 4 [52480/60000]\tLoss: 1.012385\n",
            "Train Epoch: 4 [53120/60000]\tLoss: 0.840437\n",
            "Train Epoch: 4 [53760/60000]\tLoss: 0.683705\n",
            "Train Epoch: 4 [54400/60000]\tLoss: 1.038406\n",
            "Train Epoch: 4 [55040/60000]\tLoss: 0.642628\n",
            "Train Epoch: 4 [55680/60000]\tLoss: 1.119058\n",
            "Train Epoch: 4 [56320/60000]\tLoss: 0.833794\n",
            "Train Epoch: 4 [56960/60000]\tLoss: 0.571435\n",
            "Train Epoch: 4 [57600/60000]\tLoss: 1.034019\n",
            "Train Epoch: 4 [58240/60000]\tLoss: 1.026771\n",
            "Train Epoch: 4 [58880/60000]\tLoss: 0.905452\n",
            "Train Epoch: 4 [59520/60000]\tLoss: 1.200405\n",
            "Train Epoch: 5 [0/60000]\tLoss: 0.794069\n",
            "Train Epoch: 5 [640/60000]\tLoss: 0.500728\n",
            "Train Epoch: 5 [1280/60000]\tLoss: 0.929624\n",
            "Train Epoch: 5 [1920/60000]\tLoss: 0.938299\n",
            "Train Epoch: 5 [2560/60000]\tLoss: 0.477096\n",
            "Train Epoch: 5 [3200/60000]\tLoss: 0.690159\n",
            "Train Epoch: 5 [3840/60000]\tLoss: 0.753115\n",
            "Train Epoch: 5 [4480/60000]\tLoss: 0.751571\n",
            "Train Epoch: 5 [5120/60000]\tLoss: 0.822740\n",
            "Train Epoch: 5 [5760/60000]\tLoss: 0.680285\n",
            "Train Epoch: 5 [6400/60000]\tLoss: 0.996206\n",
            "Train Epoch: 5 [7040/60000]\tLoss: 0.810694\n",
            "Train Epoch: 5 [7680/60000]\tLoss: 0.897143\n",
            "Train Epoch: 5 [8320/60000]\tLoss: 0.809566\n",
            "Train Epoch: 5 [8960/60000]\tLoss: 1.073468\n",
            "Train Epoch: 5 [9600/60000]\tLoss: 0.480772\n",
            "Train Epoch: 5 [10240/60000]\tLoss: 0.885895\n",
            "Train Epoch: 5 [10880/60000]\tLoss: 0.723420\n",
            "Train Epoch: 5 [11520/60000]\tLoss: 0.986510\n",
            "Train Epoch: 5 [12160/60000]\tLoss: 0.780773\n",
            "Train Epoch: 5 [12800/60000]\tLoss: 0.783796\n",
            "Train Epoch: 5 [13440/60000]\tLoss: 0.957034\n",
            "Train Epoch: 5 [14080/60000]\tLoss: 0.866022\n",
            "Train Epoch: 5 [14720/60000]\tLoss: 0.872042\n",
            "Train Epoch: 5 [15360/60000]\tLoss: 0.903111\n",
            "Train Epoch: 5 [16000/60000]\tLoss: 0.722264\n",
            "Train Epoch: 5 [16640/60000]\tLoss: 0.631721\n",
            "Train Epoch: 5 [17280/60000]\tLoss: 1.350100\n",
            "Train Epoch: 5 [17920/60000]\tLoss: 0.928397\n",
            "Train Epoch: 5 [18560/60000]\tLoss: 0.941249\n",
            "Train Epoch: 5 [19200/60000]\tLoss: 0.534313\n",
            "Train Epoch: 5 [19840/60000]\tLoss: 0.687168\n",
            "Train Epoch: 5 [20480/60000]\tLoss: 0.950268\n",
            "Train Epoch: 5 [21120/60000]\tLoss: 0.864804\n",
            "Train Epoch: 5 [21760/60000]\tLoss: 0.650034\n",
            "Train Epoch: 5 [22400/60000]\tLoss: 0.787414\n",
            "Train Epoch: 5 [23040/60000]\tLoss: 0.872214\n",
            "Train Epoch: 5 [23680/60000]\tLoss: 0.663310\n",
            "Train Epoch: 5 [24320/60000]\tLoss: 0.898736\n",
            "Train Epoch: 5 [24960/60000]\tLoss: 0.672859\n",
            "Train Epoch: 5 [25600/60000]\tLoss: 0.801231\n",
            "Train Epoch: 5 [26240/60000]\tLoss: 0.940308\n",
            "Train Epoch: 5 [26880/60000]\tLoss: 0.894889\n",
            "Train Epoch: 5 [27520/60000]\tLoss: 0.804222\n",
            "Train Epoch: 5 [28160/60000]\tLoss: 0.977500\n",
            "Train Epoch: 5 [28800/60000]\tLoss: 0.816117\n",
            "Train Epoch: 5 [29440/60000]\tLoss: 0.617642\n",
            "Train Epoch: 5 [30080/60000]\tLoss: 0.772610\n",
            "Train Epoch: 5 [30720/60000]\tLoss: 0.859357\n",
            "Train Epoch: 5 [31360/60000]\tLoss: 0.674365\n",
            "Train Epoch: 5 [32000/60000]\tLoss: 0.680708\n",
            "Train Epoch: 5 [32640/60000]\tLoss: 0.805479\n",
            "Train Epoch: 5 [33280/60000]\tLoss: 0.767564\n",
            "Train Epoch: 5 [33920/60000]\tLoss: 0.951643\n",
            "Train Epoch: 5 [34560/60000]\tLoss: 0.873456\n",
            "Train Epoch: 5 [35200/60000]\tLoss: 0.699416\n",
            "Train Epoch: 5 [35840/60000]\tLoss: 0.919578\n",
            "Train Epoch: 5 [36480/60000]\tLoss: 0.971363\n",
            "Train Epoch: 5 [37120/60000]\tLoss: 0.773350\n",
            "Train Epoch: 5 [37760/60000]\tLoss: 1.002966\n",
            "Train Epoch: 5 [38400/60000]\tLoss: 0.522353\n",
            "Train Epoch: 5 [39040/60000]\tLoss: 0.929982\n",
            "Train Epoch: 5 [39680/60000]\tLoss: 0.597845\n",
            "Train Epoch: 5 [40320/60000]\tLoss: 0.754077\n",
            "Train Epoch: 5 [40960/60000]\tLoss: 0.844160\n",
            "Train Epoch: 5 [41600/60000]\tLoss: 0.920244\n",
            "Train Epoch: 5 [42240/60000]\tLoss: 0.660400\n",
            "Train Epoch: 5 [42880/60000]\tLoss: 0.694895\n",
            "Train Epoch: 5 [43520/60000]\tLoss: 0.777311\n",
            "Train Epoch: 5 [44160/60000]\tLoss: 0.714453\n",
            "Train Epoch: 5 [44800/60000]\tLoss: 0.787519\n",
            "Train Epoch: 5 [45440/60000]\tLoss: 0.674329\n",
            "Train Epoch: 5 [46080/60000]\tLoss: 0.996985\n",
            "Train Epoch: 5 [46720/60000]\tLoss: 0.596517\n",
            "Train Epoch: 5 [47360/60000]\tLoss: 0.746099\n",
            "Train Epoch: 5 [48000/60000]\tLoss: 0.727144\n",
            "Train Epoch: 5 [48640/60000]\tLoss: 0.688679\n",
            "Train Epoch: 5 [49280/60000]\tLoss: 0.739760\n",
            "Train Epoch: 5 [49920/60000]\tLoss: 0.881644\n",
            "Train Epoch: 5 [50560/60000]\tLoss: 1.052438\n",
            "Train Epoch: 5 [51200/60000]\tLoss: 0.674228\n",
            "Train Epoch: 5 [51840/60000]\tLoss: 0.678600\n",
            "Train Epoch: 5 [52480/60000]\tLoss: 0.831078\n",
            "Train Epoch: 5 [53120/60000]\tLoss: 0.842197\n",
            "Train Epoch: 5 [53760/60000]\tLoss: 0.810771\n",
            "Train Epoch: 5 [54400/60000]\tLoss: 0.866826\n",
            "Train Epoch: 5 [55040/60000]\tLoss: 0.829912\n",
            "Train Epoch: 5 [55680/60000]\tLoss: 0.887450\n",
            "Train Epoch: 5 [56320/60000]\tLoss: 0.957164\n",
            "Train Epoch: 5 [56960/60000]\tLoss: 0.860519\n",
            "Train Epoch: 5 [57600/60000]\tLoss: 1.106925\n",
            "Train Epoch: 5 [58240/60000]\tLoss: 0.706100\n",
            "Train Epoch: 5 [58880/60000]\tLoss: 0.851214\n",
            "Train Epoch: 5 [59520/60000]\tLoss: 0.587073\n",
            "Train Epoch: 6 [0/60000]\tLoss: 0.723944\n",
            "Train Epoch: 6 [640/60000]\tLoss: 0.755993\n",
            "Train Epoch: 6 [1280/60000]\tLoss: 0.648573\n",
            "Train Epoch: 6 [1920/60000]\tLoss: 0.668588\n",
            "Train Epoch: 6 [2560/60000]\tLoss: 0.645630\n",
            "Train Epoch: 6 [3200/60000]\tLoss: 1.121012\n",
            "Train Epoch: 6 [3840/60000]\tLoss: 0.839373\n",
            "Train Epoch: 6 [4480/60000]\tLoss: 0.925929\n",
            "Train Epoch: 6 [5120/60000]\tLoss: 0.690837\n",
            "Train Epoch: 6 [5760/60000]\tLoss: 0.802745\n",
            "Train Epoch: 6 [6400/60000]\tLoss: 0.918707\n",
            "Train Epoch: 6 [7040/60000]\tLoss: 0.834930\n",
            "Train Epoch: 6 [7680/60000]\tLoss: 0.723578\n",
            "Train Epoch: 6 [8320/60000]\tLoss: 0.837236\n",
            "Train Epoch: 6 [8960/60000]\tLoss: 0.718208\n",
            "Train Epoch: 6 [9600/60000]\tLoss: 0.760668\n",
            "Train Epoch: 6 [10240/60000]\tLoss: 1.092950\n",
            "Train Epoch: 6 [10880/60000]\tLoss: 0.672031\n",
            "Train Epoch: 6 [11520/60000]\tLoss: 0.706688\n",
            "Train Epoch: 6 [12160/60000]\tLoss: 0.824959\n",
            "Train Epoch: 6 [12800/60000]\tLoss: 0.668924\n",
            "Train Epoch: 6 [13440/60000]\tLoss: 0.711603\n",
            "Train Epoch: 6 [14080/60000]\tLoss: 1.151787\n",
            "Train Epoch: 6 [14720/60000]\tLoss: 0.914580\n",
            "Train Epoch: 6 [15360/60000]\tLoss: 0.895646\n",
            "Train Epoch: 6 [16000/60000]\tLoss: 1.014056\n",
            "Train Epoch: 6 [16640/60000]\tLoss: 0.924301\n",
            "Train Epoch: 6 [17280/60000]\tLoss: 0.942661\n",
            "Train Epoch: 6 [17920/60000]\tLoss: 0.676287\n",
            "Train Epoch: 6 [18560/60000]\tLoss: 0.681183\n",
            "Train Epoch: 6 [19200/60000]\tLoss: 0.741905\n",
            "Train Epoch: 6 [19840/60000]\tLoss: 0.861920\n",
            "Train Epoch: 6 [20480/60000]\tLoss: 0.502166\n",
            "Train Epoch: 6 [21120/60000]\tLoss: 0.703046\n",
            "Train Epoch: 6 [21760/60000]\tLoss: 0.721548\n",
            "Train Epoch: 6 [22400/60000]\tLoss: 1.009272\n",
            "Train Epoch: 6 [23040/60000]\tLoss: 1.260954\n",
            "Train Epoch: 6 [23680/60000]\tLoss: 1.079589\n",
            "Train Epoch: 6 [24320/60000]\tLoss: 0.739871\n",
            "Train Epoch: 6 [24960/60000]\tLoss: 0.924559\n",
            "Train Epoch: 6 [25600/60000]\tLoss: 0.735277\n",
            "Train Epoch: 6 [26240/60000]\tLoss: 0.837564\n",
            "Train Epoch: 6 [26880/60000]\tLoss: 0.977839\n",
            "Train Epoch: 6 [27520/60000]\tLoss: 0.696943\n",
            "Train Epoch: 6 [28160/60000]\tLoss: 0.805981\n",
            "Train Epoch: 6 [28800/60000]\tLoss: 0.526869\n",
            "Train Epoch: 6 [29440/60000]\tLoss: 0.914818\n",
            "Train Epoch: 6 [30080/60000]\tLoss: 0.586166\n",
            "Train Epoch: 6 [30720/60000]\tLoss: 0.751933\n",
            "Train Epoch: 6 [31360/60000]\tLoss: 0.778580\n",
            "Train Epoch: 6 [32000/60000]\tLoss: 0.742343\n",
            "Train Epoch: 6 [32640/60000]\tLoss: 0.840945\n",
            "Train Epoch: 6 [33280/60000]\tLoss: 0.812664\n",
            "Train Epoch: 6 [33920/60000]\tLoss: 0.730446\n",
            "Train Epoch: 6 [34560/60000]\tLoss: 0.640632\n",
            "Train Epoch: 6 [35200/60000]\tLoss: 0.681243\n",
            "Train Epoch: 6 [35840/60000]\tLoss: 1.265082\n",
            "Train Epoch: 6 [36480/60000]\tLoss: 0.662990\n",
            "Train Epoch: 6 [37120/60000]\tLoss: 0.741506\n",
            "Train Epoch: 6 [37760/60000]\tLoss: 0.639499\n",
            "Train Epoch: 6 [38400/60000]\tLoss: 0.751054\n",
            "Train Epoch: 6 [39040/60000]\tLoss: 0.706234\n",
            "Train Epoch: 6 [39680/60000]\tLoss: 0.707338\n",
            "Train Epoch: 6 [40320/60000]\tLoss: 0.758797\n",
            "Train Epoch: 6 [40960/60000]\tLoss: 0.619597\n",
            "Train Epoch: 6 [41600/60000]\tLoss: 0.640725\n",
            "Train Epoch: 6 [42240/60000]\tLoss: 0.798977\n",
            "Train Epoch: 6 [42880/60000]\tLoss: 1.101283\n",
            "Train Epoch: 6 [43520/60000]\tLoss: 0.977680\n",
            "Train Epoch: 6 [44160/60000]\tLoss: 0.789706\n",
            "Train Epoch: 6 [44800/60000]\tLoss: 0.867822\n",
            "Train Epoch: 6 [45440/60000]\tLoss: 0.752598\n",
            "Train Epoch: 6 [46080/60000]\tLoss: 0.650031\n",
            "Train Epoch: 6 [46720/60000]\tLoss: 0.572109\n",
            "Train Epoch: 6 [47360/60000]\tLoss: 0.644361\n",
            "Train Epoch: 6 [48000/60000]\tLoss: 0.981650\n",
            "Train Epoch: 6 [48640/60000]\tLoss: 0.717612\n",
            "Train Epoch: 6 [49280/60000]\tLoss: 1.077531\n",
            "Train Epoch: 6 [49920/60000]\tLoss: 0.576213\n",
            "Train Epoch: 6 [50560/60000]\tLoss: 1.020117\n",
            "Train Epoch: 6 [51200/60000]\tLoss: 0.790890\n",
            "Train Epoch: 6 [51840/60000]\tLoss: 0.685190\n",
            "Train Epoch: 6 [52480/60000]\tLoss: 0.947940\n",
            "Train Epoch: 6 [53120/60000]\tLoss: 0.881576\n",
            "Train Epoch: 6 [53760/60000]\tLoss: 0.850708\n",
            "Train Epoch: 6 [54400/60000]\tLoss: 0.780470\n",
            "Train Epoch: 6 [55040/60000]\tLoss: 0.854637\n",
            "Train Epoch: 6 [55680/60000]\tLoss: 0.818548\n",
            "Train Epoch: 6 [56320/60000]\tLoss: 1.075957\n",
            "Train Epoch: 6 [56960/60000]\tLoss: 0.699024\n",
            "Train Epoch: 6 [57600/60000]\tLoss: 0.780169\n",
            "Train Epoch: 6 [58240/60000]\tLoss: 0.707910\n",
            "Train Epoch: 6 [58880/60000]\tLoss: 0.895587\n",
            "Train Epoch: 6 [59520/60000]\tLoss: 0.796578\n",
            "Train Epoch: 7 [0/60000]\tLoss: 0.571084\n",
            "Train Epoch: 7 [640/60000]\tLoss: 0.871419\n",
            "Train Epoch: 7 [1280/60000]\tLoss: 0.888354\n",
            "Train Epoch: 7 [1920/60000]\tLoss: 0.665986\n",
            "Train Epoch: 7 [2560/60000]\tLoss: 0.844034\n",
            "Train Epoch: 7 [3200/60000]\tLoss: 0.864440\n",
            "Train Epoch: 7 [3840/60000]\tLoss: 1.304482\n",
            "Train Epoch: 7 [4480/60000]\tLoss: 0.801577\n",
            "Train Epoch: 7 [5120/60000]\tLoss: 0.736815\n",
            "Train Epoch: 7 [5760/60000]\tLoss: 0.827091\n",
            "Train Epoch: 7 [6400/60000]\tLoss: 0.930585\n",
            "Train Epoch: 7 [7040/60000]\tLoss: 0.842067\n",
            "Train Epoch: 7 [7680/60000]\tLoss: 0.803648\n",
            "Train Epoch: 7 [8320/60000]\tLoss: 0.919782\n",
            "Train Epoch: 7 [8960/60000]\tLoss: 0.819736\n",
            "Train Epoch: 7 [9600/60000]\tLoss: 0.743758\n",
            "Train Epoch: 7 [10240/60000]\tLoss: 0.871052\n",
            "Train Epoch: 7 [10880/60000]\tLoss: 0.733565\n",
            "Train Epoch: 7 [11520/60000]\tLoss: 0.795355\n",
            "Train Epoch: 7 [12160/60000]\tLoss: 0.689751\n",
            "Train Epoch: 7 [12800/60000]\tLoss: 0.943049\n",
            "Train Epoch: 7 [13440/60000]\tLoss: 0.855653\n",
            "Train Epoch: 7 [14080/60000]\tLoss: 0.897155\n",
            "Train Epoch: 7 [14720/60000]\tLoss: 0.749213\n",
            "Train Epoch: 7 [15360/60000]\tLoss: 1.058954\n",
            "Train Epoch: 7 [16000/60000]\tLoss: 0.669377\n",
            "Train Epoch: 7 [16640/60000]\tLoss: 0.795310\n",
            "Train Epoch: 7 [17280/60000]\tLoss: 1.002523\n",
            "Train Epoch: 7 [17920/60000]\tLoss: 0.716209\n",
            "Train Epoch: 7 [18560/60000]\tLoss: 0.475250\n",
            "Train Epoch: 7 [19200/60000]\tLoss: 0.944765\n",
            "Train Epoch: 7 [19840/60000]\tLoss: 0.961476\n",
            "Train Epoch: 7 [20480/60000]\tLoss: 0.895411\n",
            "Train Epoch: 7 [21120/60000]\tLoss: 0.944363\n",
            "Train Epoch: 7 [21760/60000]\tLoss: 0.886304\n",
            "Train Epoch: 7 [22400/60000]\tLoss: 0.794117\n",
            "Train Epoch: 7 [23040/60000]\tLoss: 0.537745\n",
            "Train Epoch: 7 [23680/60000]\tLoss: 0.764547\n",
            "Train Epoch: 7 [24320/60000]\tLoss: 0.661031\n",
            "Train Epoch: 7 [24960/60000]\tLoss: 0.699229\n",
            "Train Epoch: 7 [25600/60000]\tLoss: 0.528671\n",
            "Train Epoch: 7 [26240/60000]\tLoss: 0.898140\n",
            "Train Epoch: 7 [26880/60000]\tLoss: 1.062035\n",
            "Train Epoch: 7 [27520/60000]\tLoss: 0.892871\n",
            "Train Epoch: 7 [28160/60000]\tLoss: 1.079297\n",
            "Train Epoch: 7 [28800/60000]\tLoss: 0.796928\n",
            "Train Epoch: 7 [29440/60000]\tLoss: 0.994001\n",
            "Train Epoch: 7 [30080/60000]\tLoss: 0.752667\n",
            "Train Epoch: 7 [30720/60000]\tLoss: 0.971328\n",
            "Train Epoch: 7 [31360/60000]\tLoss: 0.752971\n",
            "Train Epoch: 7 [32000/60000]\tLoss: 0.806704\n",
            "Train Epoch: 7 [32640/60000]\tLoss: 0.948952\n",
            "Train Epoch: 7 [33280/60000]\tLoss: 0.879501\n",
            "Train Epoch: 7 [33920/60000]\tLoss: 0.849927\n",
            "Train Epoch: 7 [34560/60000]\tLoss: 0.752174\n",
            "Train Epoch: 7 [35200/60000]\tLoss: 0.850180\n",
            "Train Epoch: 7 [35840/60000]\tLoss: 0.938149\n",
            "Train Epoch: 7 [36480/60000]\tLoss: 0.546534\n",
            "Train Epoch: 7 [37120/60000]\tLoss: 0.827220\n",
            "Train Epoch: 7 [37760/60000]\tLoss: 0.979959\n",
            "Train Epoch: 7 [38400/60000]\tLoss: 0.816112\n",
            "Train Epoch: 7 [39040/60000]\tLoss: 0.968752\n",
            "Train Epoch: 7 [39680/60000]\tLoss: 0.750471\n",
            "Train Epoch: 7 [40320/60000]\tLoss: 0.765033\n",
            "Train Epoch: 7 [40960/60000]\tLoss: 0.741222\n",
            "Train Epoch: 7 [41600/60000]\tLoss: 0.762755\n",
            "Train Epoch: 7 [42240/60000]\tLoss: 0.697623\n",
            "Train Epoch: 7 [42880/60000]\tLoss: 0.748066\n",
            "Train Epoch: 7 [43520/60000]\tLoss: 0.823727\n",
            "Train Epoch: 7 [44160/60000]\tLoss: 0.822357\n",
            "Train Epoch: 7 [44800/60000]\tLoss: 0.577821\n",
            "Train Epoch: 7 [45440/60000]\tLoss: 0.854326\n",
            "Train Epoch: 7 [46080/60000]\tLoss: 0.840959\n",
            "Train Epoch: 7 [46720/60000]\tLoss: 0.674304\n",
            "Train Epoch: 7 [47360/60000]\tLoss: 0.738959\n",
            "Train Epoch: 7 [48000/60000]\tLoss: 0.707681\n",
            "Train Epoch: 7 [48640/60000]\tLoss: 0.796313\n",
            "Train Epoch: 7 [49280/60000]\tLoss: 0.834213\n",
            "Train Epoch: 7 [49920/60000]\tLoss: 0.949828\n",
            "Train Epoch: 7 [50560/60000]\tLoss: 0.751633\n",
            "Train Epoch: 7 [51200/60000]\tLoss: 0.725370\n",
            "Train Epoch: 7 [51840/60000]\tLoss: 0.749444\n",
            "Train Epoch: 7 [52480/60000]\tLoss: 0.788843\n",
            "Train Epoch: 7 [53120/60000]\tLoss: 1.068691\n",
            "Train Epoch: 7 [53760/60000]\tLoss: 0.886386\n",
            "Train Epoch: 7 [54400/60000]\tLoss: 0.640938\n",
            "Train Epoch: 7 [55040/60000]\tLoss: 0.706791\n",
            "Train Epoch: 7 [55680/60000]\tLoss: 1.242295\n",
            "Train Epoch: 7 [56320/60000]\tLoss: 0.897797\n",
            "Train Epoch: 7 [56960/60000]\tLoss: 0.667555\n",
            "Train Epoch: 7 [57600/60000]\tLoss: 0.969329\n",
            "Train Epoch: 7 [58240/60000]\tLoss: 0.703701\n",
            "Train Epoch: 7 [58880/60000]\tLoss: 0.933988\n",
            "Train Epoch: 7 [59520/60000]\tLoss: 0.766362\n",
            "Train Epoch: 8 [0/60000]\tLoss: 0.858188\n",
            "Train Epoch: 8 [640/60000]\tLoss: 0.724832\n",
            "Train Epoch: 8 [1280/60000]\tLoss: 0.679410\n",
            "Train Epoch: 8 [1920/60000]\tLoss: 0.765976\n",
            "Train Epoch: 8 [2560/60000]\tLoss: 0.718457\n",
            "Train Epoch: 8 [3200/60000]\tLoss: 0.922493\n",
            "Train Epoch: 8 [3840/60000]\tLoss: 0.958944\n",
            "Train Epoch: 8 [4480/60000]\tLoss: 0.779950\n",
            "Train Epoch: 8 [5120/60000]\tLoss: 0.761006\n",
            "Train Epoch: 8 [5760/60000]\tLoss: 0.785538\n",
            "Train Epoch: 8 [6400/60000]\tLoss: 0.663809\n",
            "Train Epoch: 8 [7040/60000]\tLoss: 0.937602\n",
            "Train Epoch: 8 [7680/60000]\tLoss: 0.712146\n",
            "Train Epoch: 8 [8320/60000]\tLoss: 0.699600\n",
            "Train Epoch: 8 [8960/60000]\tLoss: 0.965268\n",
            "Train Epoch: 8 [9600/60000]\tLoss: 0.778174\n",
            "Train Epoch: 8 [10240/60000]\tLoss: 0.868636\n",
            "Train Epoch: 8 [10880/60000]\tLoss: 0.751282\n",
            "Train Epoch: 8 [11520/60000]\tLoss: 0.763484\n",
            "Train Epoch: 8 [12160/60000]\tLoss: 0.625092\n",
            "Train Epoch: 8 [12800/60000]\tLoss: 0.812007\n",
            "Train Epoch: 8 [13440/60000]\tLoss: 0.686125\n",
            "Train Epoch: 8 [14080/60000]\tLoss: 0.547981\n",
            "Train Epoch: 8 [14720/60000]\tLoss: 1.203749\n",
            "Train Epoch: 8 [15360/60000]\tLoss: 0.665597\n",
            "Train Epoch: 8 [16000/60000]\tLoss: 0.803494\n",
            "Train Epoch: 8 [16640/60000]\tLoss: 0.993704\n",
            "Train Epoch: 8 [17280/60000]\tLoss: 0.866957\n",
            "Train Epoch: 8 [17920/60000]\tLoss: 0.854386\n",
            "Train Epoch: 8 [18560/60000]\tLoss: 0.549007\n",
            "Train Epoch: 8 [19200/60000]\tLoss: 1.105951\n",
            "Train Epoch: 8 [19840/60000]\tLoss: 0.716385\n",
            "Train Epoch: 8 [20480/60000]\tLoss: 0.857574\n",
            "Train Epoch: 8 [21120/60000]\tLoss: 1.006366\n",
            "Train Epoch: 8 [21760/60000]\tLoss: 0.927439\n",
            "Train Epoch: 8 [22400/60000]\tLoss: 0.556710\n",
            "Train Epoch: 8 [23040/60000]\tLoss: 0.788634\n",
            "Train Epoch: 8 [23680/60000]\tLoss: 0.671426\n",
            "Train Epoch: 8 [24320/60000]\tLoss: 0.803532\n",
            "Train Epoch: 8 [24960/60000]\tLoss: 0.649188\n",
            "Train Epoch: 8 [25600/60000]\tLoss: 0.749396\n",
            "Train Epoch: 8 [26240/60000]\tLoss: 0.772915\n",
            "Train Epoch: 8 [26880/60000]\tLoss: 0.987610\n",
            "Train Epoch: 8 [27520/60000]\tLoss: 0.809192\n",
            "Train Epoch: 8 [28160/60000]\tLoss: 0.954305\n",
            "Train Epoch: 8 [28800/60000]\tLoss: 0.733608\n",
            "Train Epoch: 8 [29440/60000]\tLoss: 0.924105\n",
            "Train Epoch: 8 [30080/60000]\tLoss: 0.825314\n",
            "Train Epoch: 8 [30720/60000]\tLoss: 0.716706\n",
            "Train Epoch: 8 [31360/60000]\tLoss: 0.676505\n",
            "Train Epoch: 8 [32000/60000]\tLoss: 1.035199\n",
            "Train Epoch: 8 [32640/60000]\tLoss: 0.860925\n",
            "Train Epoch: 8 [33280/60000]\tLoss: 0.823096\n",
            "Train Epoch: 8 [33920/60000]\tLoss: 0.982956\n",
            "Train Epoch: 8 [34560/60000]\tLoss: 0.844231\n",
            "Train Epoch: 8 [35200/60000]\tLoss: 0.839138\n",
            "Train Epoch: 8 [35840/60000]\tLoss: 0.589633\n",
            "Train Epoch: 8 [36480/60000]\tLoss: 0.668091\n",
            "Train Epoch: 8 [37120/60000]\tLoss: 0.914256\n",
            "Train Epoch: 8 [37760/60000]\tLoss: 0.880836\n",
            "Train Epoch: 8 [38400/60000]\tLoss: 0.654300\n",
            "Train Epoch: 8 [39040/60000]\tLoss: 0.829789\n",
            "Train Epoch: 8 [39680/60000]\tLoss: 0.790998\n",
            "Train Epoch: 8 [40320/60000]\tLoss: 0.909574\n",
            "Train Epoch: 8 [40960/60000]\tLoss: 0.770591\n",
            "Train Epoch: 8 [41600/60000]\tLoss: 0.751135\n",
            "Train Epoch: 8 [42240/60000]\tLoss: 0.850572\n",
            "Train Epoch: 8 [42880/60000]\tLoss: 0.860015\n",
            "Train Epoch: 8 [43520/60000]\tLoss: 0.736152\n",
            "Train Epoch: 8 [44160/60000]\tLoss: 0.641625\n",
            "Train Epoch: 8 [44800/60000]\tLoss: 0.696886\n",
            "Train Epoch: 8 [45440/60000]\tLoss: 0.864203\n",
            "Train Epoch: 8 [46080/60000]\tLoss: 0.833219\n",
            "Train Epoch: 8 [46720/60000]\tLoss: 0.829417\n",
            "Train Epoch: 8 [47360/60000]\tLoss: 0.842224\n",
            "Train Epoch: 8 [48000/60000]\tLoss: 0.722849\n",
            "Train Epoch: 8 [48640/60000]\tLoss: 0.864439\n",
            "Train Epoch: 8 [49280/60000]\tLoss: 0.737181\n",
            "Train Epoch: 8 [49920/60000]\tLoss: 0.951486\n",
            "Train Epoch: 8 [50560/60000]\tLoss: 0.566245\n",
            "Train Epoch: 8 [51200/60000]\tLoss: 0.618360\n",
            "Train Epoch: 8 [51840/60000]\tLoss: 0.963531\n",
            "Train Epoch: 8 [52480/60000]\tLoss: 0.752835\n",
            "Train Epoch: 8 [53120/60000]\tLoss: 0.785976\n",
            "Train Epoch: 8 [53760/60000]\tLoss: 0.624067\n",
            "Train Epoch: 8 [54400/60000]\tLoss: 0.905836\n",
            "Train Epoch: 8 [55040/60000]\tLoss: 0.804209\n",
            "Train Epoch: 8 [55680/60000]\tLoss: 0.727473\n",
            "Train Epoch: 8 [56320/60000]\tLoss: 0.653247\n",
            "Train Epoch: 8 [56960/60000]\tLoss: 1.025046\n",
            "Train Epoch: 8 [57600/60000]\tLoss: 0.551273\n",
            "Train Epoch: 8 [58240/60000]\tLoss: 1.018969\n",
            "Train Epoch: 8 [58880/60000]\tLoss: 0.716755\n",
            "Train Epoch: 8 [59520/60000]\tLoss: 0.633396\n",
            "Train Epoch: 9 [0/60000]\tLoss: 0.715180\n",
            "Train Epoch: 9 [640/60000]\tLoss: 0.832173\n",
            "Train Epoch: 9 [1280/60000]\tLoss: 0.562521\n",
            "Train Epoch: 9 [1920/60000]\tLoss: 0.504875\n",
            "Train Epoch: 9 [2560/60000]\tLoss: 0.697691\n",
            "Train Epoch: 9 [3200/60000]\tLoss: 0.784298\n",
            "Train Epoch: 9 [3840/60000]\tLoss: 0.781982\n",
            "Train Epoch: 9 [4480/60000]\tLoss: 0.886613\n",
            "Train Epoch: 9 [5120/60000]\tLoss: 0.949307\n",
            "Train Epoch: 9 [5760/60000]\tLoss: 0.624298\n",
            "Train Epoch: 9 [6400/60000]\tLoss: 0.998285\n",
            "Train Epoch: 9 [7040/60000]\tLoss: 0.825272\n",
            "Train Epoch: 9 [7680/60000]\tLoss: 0.945086\n",
            "Train Epoch: 9 [8320/60000]\tLoss: 0.681451\n",
            "Train Epoch: 9 [8960/60000]\tLoss: 0.690418\n",
            "Train Epoch: 9 [9600/60000]\tLoss: 0.644835\n",
            "Train Epoch: 9 [10240/60000]\tLoss: 0.840727\n",
            "Train Epoch: 9 [10880/60000]\tLoss: 0.792802\n",
            "Train Epoch: 9 [11520/60000]\tLoss: 0.781369\n",
            "Train Epoch: 9 [12160/60000]\tLoss: 0.941733\n",
            "Train Epoch: 9 [12800/60000]\tLoss: 0.553682\n",
            "Train Epoch: 9 [13440/60000]\tLoss: 0.651209\n",
            "Train Epoch: 9 [14080/60000]\tLoss: 0.849768\n",
            "Train Epoch: 9 [14720/60000]\tLoss: 0.677983\n",
            "Train Epoch: 9 [15360/60000]\tLoss: 0.633552\n",
            "Train Epoch: 9 [16000/60000]\tLoss: 0.752022\n",
            "Train Epoch: 9 [16640/60000]\tLoss: 0.870723\n",
            "Train Epoch: 9 [17280/60000]\tLoss: 0.690581\n",
            "Train Epoch: 9 [17920/60000]\tLoss: 0.701485\n",
            "Train Epoch: 9 [18560/60000]\tLoss: 1.345013\n",
            "Train Epoch: 9 [19200/60000]\tLoss: 0.686189\n",
            "Train Epoch: 9 [19840/60000]\tLoss: 0.937879\n",
            "Train Epoch: 9 [20480/60000]\tLoss: 0.742567\n",
            "Train Epoch: 9 [21120/60000]\tLoss: 0.640294\n",
            "Train Epoch: 9 [21760/60000]\tLoss: 0.568431\n",
            "Train Epoch: 9 [22400/60000]\tLoss: 0.778056\n",
            "Train Epoch: 9 [23040/60000]\tLoss: 0.681662\n",
            "Train Epoch: 9 [23680/60000]\tLoss: 0.810995\n",
            "Train Epoch: 9 [24320/60000]\tLoss: 0.635350\n",
            "Train Epoch: 9 [24960/60000]\tLoss: 0.702254\n",
            "Train Epoch: 9 [25600/60000]\tLoss: 0.865353\n",
            "Train Epoch: 9 [26240/60000]\tLoss: 0.461784\n",
            "Train Epoch: 9 [26880/60000]\tLoss: 0.965152\n",
            "Train Epoch: 9 [27520/60000]\tLoss: 0.829721\n",
            "Train Epoch: 9 [28160/60000]\tLoss: 0.693458\n",
            "Train Epoch: 9 [28800/60000]\tLoss: 0.834936\n",
            "Train Epoch: 9 [29440/60000]\tLoss: 1.042181\n",
            "Train Epoch: 9 [30080/60000]\tLoss: 1.024744\n",
            "Train Epoch: 9 [30720/60000]\tLoss: 0.612857\n",
            "Train Epoch: 9 [31360/60000]\tLoss: 0.567882\n",
            "Train Epoch: 9 [32000/60000]\tLoss: 0.825763\n",
            "Train Epoch: 9 [32640/60000]\tLoss: 0.894992\n",
            "Train Epoch: 9 [33280/60000]\tLoss: 0.912455\n",
            "Train Epoch: 9 [33920/60000]\tLoss: 1.035652\n",
            "Train Epoch: 9 [34560/60000]\tLoss: 0.687318\n",
            "Train Epoch: 9 [35200/60000]\tLoss: 0.770572\n",
            "Train Epoch: 9 [35840/60000]\tLoss: 1.053448\n",
            "Train Epoch: 9 [36480/60000]\tLoss: 0.849219\n",
            "Train Epoch: 9 [37120/60000]\tLoss: 0.989070\n",
            "Train Epoch: 9 [37760/60000]\tLoss: 0.810840\n",
            "Train Epoch: 9 [38400/60000]\tLoss: 0.820510\n",
            "Train Epoch: 9 [39040/60000]\tLoss: 0.833112\n",
            "Train Epoch: 9 [39680/60000]\tLoss: 0.682457\n",
            "Train Epoch: 9 [40320/60000]\tLoss: 0.705702\n",
            "Train Epoch: 9 [40960/60000]\tLoss: 0.763454\n",
            "Train Epoch: 9 [41600/60000]\tLoss: 0.832876\n",
            "Train Epoch: 9 [42240/60000]\tLoss: 1.061005\n",
            "Train Epoch: 9 [42880/60000]\tLoss: 0.956095\n",
            "Train Epoch: 9 [43520/60000]\tLoss: 0.785662\n",
            "Train Epoch: 9 [44160/60000]\tLoss: 0.931970\n",
            "Train Epoch: 9 [44800/60000]\tLoss: 0.971352\n",
            "Train Epoch: 9 [45440/60000]\tLoss: 0.762656\n",
            "Train Epoch: 9 [46080/60000]\tLoss: 0.727674\n",
            "Train Epoch: 9 [46720/60000]\tLoss: 0.918183\n",
            "Train Epoch: 9 [47360/60000]\tLoss: 0.733978\n",
            "Train Epoch: 9 [48000/60000]\tLoss: 0.668124\n",
            "Train Epoch: 9 [48640/60000]\tLoss: 0.952624\n",
            "Train Epoch: 9 [49280/60000]\tLoss: 0.644741\n",
            "Train Epoch: 9 [49920/60000]\tLoss: 0.689901\n",
            "Train Epoch: 9 [50560/60000]\tLoss: 0.809298\n",
            "Train Epoch: 9 [51200/60000]\tLoss: 0.851180\n",
            "Train Epoch: 9 [51840/60000]\tLoss: 0.949516\n",
            "Train Epoch: 9 [52480/60000]\tLoss: 0.904303\n",
            "Train Epoch: 9 [53120/60000]\tLoss: 0.583892\n",
            "Train Epoch: 9 [53760/60000]\tLoss: 0.907809\n",
            "Train Epoch: 9 [54400/60000]\tLoss: 0.858980\n",
            "Train Epoch: 9 [55040/60000]\tLoss: 0.615571\n",
            "Train Epoch: 9 [55680/60000]\tLoss: 1.108986\n",
            "Train Epoch: 9 [56320/60000]\tLoss: 0.637430\n",
            "Train Epoch: 9 [56960/60000]\tLoss: 0.552453\n",
            "Train Epoch: 9 [57600/60000]\tLoss: 0.897958\n",
            "Train Epoch: 9 [58240/60000]\tLoss: 0.551858\n",
            "Train Epoch: 9 [58880/60000]\tLoss: 0.558716\n",
            "Train Epoch: 9 [59520/60000]\tLoss: 1.073538\n",
            "Train Epoch: 10 [0/60000]\tLoss: 0.668320\n",
            "Train Epoch: 10 [640/60000]\tLoss: 0.805880\n",
            "Train Epoch: 10 [1280/60000]\tLoss: 0.922641\n",
            "Train Epoch: 10 [1920/60000]\tLoss: 0.780904\n",
            "Train Epoch: 10 [2560/60000]\tLoss: 0.792068\n",
            "Train Epoch: 10 [3200/60000]\tLoss: 0.602565\n",
            "Train Epoch: 10 [3840/60000]\tLoss: 0.645135\n",
            "Train Epoch: 10 [4480/60000]\tLoss: 1.108476\n",
            "Train Epoch: 10 [5120/60000]\tLoss: 0.677792\n",
            "Train Epoch: 10 [5760/60000]\tLoss: 0.650689\n",
            "Train Epoch: 10 [6400/60000]\tLoss: 0.741735\n",
            "Train Epoch: 10 [7040/60000]\tLoss: 0.638007\n",
            "Train Epoch: 10 [7680/60000]\tLoss: 0.800751\n",
            "Train Epoch: 10 [8320/60000]\tLoss: 0.651560\n",
            "Train Epoch: 10 [8960/60000]\tLoss: 0.687974\n",
            "Train Epoch: 10 [9600/60000]\tLoss: 0.719071\n",
            "Train Epoch: 10 [10240/60000]\tLoss: 0.776622\n",
            "Train Epoch: 10 [10880/60000]\tLoss: 0.561657\n",
            "Train Epoch: 10 [11520/60000]\tLoss: 0.897429\n",
            "Train Epoch: 10 [12160/60000]\tLoss: 0.762606\n",
            "Train Epoch: 10 [12800/60000]\tLoss: 1.006839\n",
            "Train Epoch: 10 [13440/60000]\tLoss: 0.643585\n",
            "Train Epoch: 10 [14080/60000]\tLoss: 0.994386\n",
            "Train Epoch: 10 [14720/60000]\tLoss: 0.592726\n",
            "Train Epoch: 10 [15360/60000]\tLoss: 0.661881\n",
            "Train Epoch: 10 [16000/60000]\tLoss: 0.625687\n",
            "Train Epoch: 10 [16640/60000]\tLoss: 0.954405\n",
            "Train Epoch: 10 [17280/60000]\tLoss: 0.683413\n",
            "Train Epoch: 10 [17920/60000]\tLoss: 0.858224\n",
            "Train Epoch: 10 [18560/60000]\tLoss: 0.786077\n",
            "Train Epoch: 10 [19200/60000]\tLoss: 0.791981\n",
            "Train Epoch: 10 [19840/60000]\tLoss: 0.877097\n",
            "Train Epoch: 10 [20480/60000]\tLoss: 0.751081\n",
            "Train Epoch: 10 [21120/60000]\tLoss: 0.771881\n",
            "Train Epoch: 10 [21760/60000]\tLoss: 0.857373\n",
            "Train Epoch: 10 [22400/60000]\tLoss: 0.862204\n",
            "Train Epoch: 10 [23040/60000]\tLoss: 0.695589\n",
            "Train Epoch: 10 [23680/60000]\tLoss: 0.743334\n",
            "Train Epoch: 10 [24320/60000]\tLoss: 0.605791\n",
            "Train Epoch: 10 [24960/60000]\tLoss: 0.745590\n",
            "Train Epoch: 10 [25600/60000]\tLoss: 0.965372\n",
            "Train Epoch: 10 [26240/60000]\tLoss: 0.733352\n",
            "Train Epoch: 10 [26880/60000]\tLoss: 0.832313\n",
            "Train Epoch: 10 [27520/60000]\tLoss: 0.770050\n",
            "Train Epoch: 10 [28160/60000]\tLoss: 1.069008\n",
            "Train Epoch: 10 [28800/60000]\tLoss: 1.132388\n",
            "Train Epoch: 10 [29440/60000]\tLoss: 0.702924\n",
            "Train Epoch: 10 [30080/60000]\tLoss: 0.785574\n",
            "Train Epoch: 10 [30720/60000]\tLoss: 0.992448\n",
            "Train Epoch: 10 [31360/60000]\tLoss: 1.080550\n",
            "Train Epoch: 10 [32000/60000]\tLoss: 0.961927\n",
            "Train Epoch: 10 [32640/60000]\tLoss: 1.128276\n",
            "Train Epoch: 10 [33280/60000]\tLoss: 0.841227\n",
            "Train Epoch: 10 [33920/60000]\tLoss: 0.636001\n",
            "Train Epoch: 10 [34560/60000]\tLoss: 0.868956\n",
            "Train Epoch: 10 [35200/60000]\tLoss: 0.752930\n",
            "Train Epoch: 10 [35840/60000]\tLoss: 0.691367\n",
            "Train Epoch: 10 [36480/60000]\tLoss: 0.866894\n",
            "Train Epoch: 10 [37120/60000]\tLoss: 0.768730\n",
            "Train Epoch: 10 [37760/60000]\tLoss: 0.874741\n",
            "Train Epoch: 10 [38400/60000]\tLoss: 0.727193\n",
            "Train Epoch: 10 [39040/60000]\tLoss: 1.059664\n",
            "Train Epoch: 10 [39680/60000]\tLoss: 0.635775\n",
            "Train Epoch: 10 [40320/60000]\tLoss: 0.787150\n",
            "Train Epoch: 10 [40960/60000]\tLoss: 1.073447\n",
            "Train Epoch: 10 [41600/60000]\tLoss: 0.751686\n",
            "Train Epoch: 10 [42240/60000]\tLoss: 0.873076\n",
            "Train Epoch: 10 [42880/60000]\tLoss: 0.565496\n",
            "Train Epoch: 10 [43520/60000]\tLoss: 0.734884\n",
            "Train Epoch: 10 [44160/60000]\tLoss: 0.947870\n",
            "Train Epoch: 10 [44800/60000]\tLoss: 0.947818\n",
            "Train Epoch: 10 [45440/60000]\tLoss: 0.379566\n",
            "Train Epoch: 10 [46080/60000]\tLoss: 0.795756\n",
            "Train Epoch: 10 [46720/60000]\tLoss: 0.576230\n",
            "Train Epoch: 10 [47360/60000]\tLoss: 0.795707\n",
            "Train Epoch: 10 [48000/60000]\tLoss: 0.896432\n",
            "Train Epoch: 10 [48640/60000]\tLoss: 0.734945\n",
            "Train Epoch: 10 [49280/60000]\tLoss: 1.133267\n",
            "Train Epoch: 10 [49920/60000]\tLoss: 0.868905\n",
            "Train Epoch: 10 [50560/60000]\tLoss: 0.939293\n",
            "Train Epoch: 10 [51200/60000]\tLoss: 0.888227\n",
            "Train Epoch: 10 [51840/60000]\tLoss: 0.693945\n",
            "Train Epoch: 10 [52480/60000]\tLoss: 0.948979\n",
            "Train Epoch: 10 [53120/60000]\tLoss: 0.777681\n",
            "Train Epoch: 10 [53760/60000]\tLoss: 0.873846\n",
            "Train Epoch: 10 [54400/60000]\tLoss: 0.716968\n",
            "Train Epoch: 10 [55040/60000]\tLoss: 0.929672\n",
            "Train Epoch: 10 [55680/60000]\tLoss: 0.681285\n",
            "Train Epoch: 10 [56320/60000]\tLoss: 0.591140\n",
            "Train Epoch: 10 [56960/60000]\tLoss: 1.005333\n",
            "Train Epoch: 10 [57600/60000]\tLoss: 0.890413\n",
            "Train Epoch: 10 [58240/60000]\tLoss: 0.802418\n",
            "Train Epoch: 10 [58880/60000]\tLoss: 0.866236\n",
            "Train Epoch: 10 [59520/60000]\tLoss: 0.868276\n",
            "\n",
            "Test set: Avg. loss: 0.2685, Accuracy: 9238/10000 (92%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train and test Model 1\n",
        "\n",
        "# Create network\n",
        "model = Net()\n",
        "# Initialize model weights\n",
        "model.apply(weights_init)\n",
        "# Define optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "# Get initial performance\n",
        "test(model)\n",
        "# Train for ten epochs\n",
        "n_epochs = 10\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(epoch, model)\n",
        "accuracy1 = test(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2PVnghrmr0F",
        "outputId": "d9d7e075-1371-46c3-fddf-9fc840ea190d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-b4df5afe933b>:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Avg. loss: 2.4300, Accuracy: 779/10000 (8%)\n",
            "\n",
            "Train Epoch: 1 [0/60000]\tLoss: 2.610041\n",
            "Train Epoch: 1 [640/60000]\tLoss: 2.191328\n",
            "Train Epoch: 1 [1280/60000]\tLoss: 2.002594\n",
            "Train Epoch: 1 [1920/60000]\tLoss: 1.763404\n",
            "Train Epoch: 1 [2560/60000]\tLoss: 1.595986\n",
            "Train Epoch: 1 [3200/60000]\tLoss: 1.545032\n",
            "Train Epoch: 1 [3840/60000]\tLoss: 1.195713\n",
            "Train Epoch: 1 [4480/60000]\tLoss: 1.107968\n",
            "Train Epoch: 1 [5120/60000]\tLoss: 1.342009\n",
            "Train Epoch: 1 [5760/60000]\tLoss: 0.878921\n",
            "Train Epoch: 1 [6400/60000]\tLoss: 0.747351\n",
            "Train Epoch: 1 [7040/60000]\tLoss: 1.256157\n",
            "Train Epoch: 1 [7680/60000]\tLoss: 0.648553\n",
            "Train Epoch: 1 [8320/60000]\tLoss: 0.473850\n",
            "Train Epoch: 1 [8960/60000]\tLoss: 0.727336\n",
            "Train Epoch: 1 [9600/60000]\tLoss: 0.706552\n",
            "Train Epoch: 1 [10240/60000]\tLoss: 0.633360\n",
            "Train Epoch: 1 [10880/60000]\tLoss: 0.445729\n",
            "Train Epoch: 1 [11520/60000]\tLoss: 0.670080\n",
            "Train Epoch: 1 [12160/60000]\tLoss: 0.693154\n",
            "Train Epoch: 1 [12800/60000]\tLoss: 0.603127\n",
            "Train Epoch: 1 [13440/60000]\tLoss: 0.648038\n",
            "Train Epoch: 1 [14080/60000]\tLoss: 0.445715\n",
            "Train Epoch: 1 [14720/60000]\tLoss: 0.519353\n",
            "Train Epoch: 1 [15360/60000]\tLoss: 0.578525\n",
            "Train Epoch: 1 [16000/60000]\tLoss: 0.370530\n",
            "Train Epoch: 1 [16640/60000]\tLoss: 0.349029\n",
            "Train Epoch: 1 [17280/60000]\tLoss: 0.328340\n",
            "Train Epoch: 1 [17920/60000]\tLoss: 0.349999\n",
            "Train Epoch: 1 [18560/60000]\tLoss: 0.456417\n",
            "Train Epoch: 1 [19200/60000]\tLoss: 0.312330\n",
            "Train Epoch: 1 [19840/60000]\tLoss: 0.488331\n",
            "Train Epoch: 1 [20480/60000]\tLoss: 0.366399\n",
            "Train Epoch: 1 [21120/60000]\tLoss: 0.384733\n",
            "Train Epoch: 1 [21760/60000]\tLoss: 0.427656\n",
            "Train Epoch: 1 [22400/60000]\tLoss: 0.239942\n",
            "Train Epoch: 1 [23040/60000]\tLoss: 0.351374\n",
            "Train Epoch: 1 [23680/60000]\tLoss: 0.346598\n",
            "Train Epoch: 1 [24320/60000]\tLoss: 0.453553\n",
            "Train Epoch: 1 [24960/60000]\tLoss: 0.342180\n",
            "Train Epoch: 1 [25600/60000]\tLoss: 0.415689\n",
            "Train Epoch: 1 [26240/60000]\tLoss: 0.267086\n",
            "Train Epoch: 1 [26880/60000]\tLoss: 0.356616\n",
            "Train Epoch: 1 [27520/60000]\tLoss: 0.378542\n",
            "Train Epoch: 1 [28160/60000]\tLoss: 0.308881\n",
            "Train Epoch: 1 [28800/60000]\tLoss: 0.366674\n",
            "Train Epoch: 1 [29440/60000]\tLoss: 0.249403\n",
            "Train Epoch: 1 [30080/60000]\tLoss: 0.269041\n",
            "Train Epoch: 1 [30720/60000]\tLoss: 0.295094\n",
            "Train Epoch: 1 [31360/60000]\tLoss: 0.283224\n",
            "Train Epoch: 1 [32000/60000]\tLoss: 0.299378\n",
            "Train Epoch: 1 [32640/60000]\tLoss: 0.196229\n",
            "Train Epoch: 1 [33280/60000]\tLoss: 0.298763\n",
            "Train Epoch: 1 [33920/60000]\tLoss: 0.479031\n",
            "Train Epoch: 1 [34560/60000]\tLoss: 0.419244\n",
            "Train Epoch: 1 [35200/60000]\tLoss: 0.367839\n",
            "Train Epoch: 1 [35840/60000]\tLoss: 0.225894\n",
            "Train Epoch: 1 [36480/60000]\tLoss: 0.305438\n",
            "Train Epoch: 1 [37120/60000]\tLoss: 0.346511\n",
            "Train Epoch: 1 [37760/60000]\tLoss: 0.324217\n",
            "Train Epoch: 1 [38400/60000]\tLoss: 0.227927\n",
            "Train Epoch: 1 [39040/60000]\tLoss: 0.243942\n",
            "Train Epoch: 1 [39680/60000]\tLoss: 0.328621\n",
            "Train Epoch: 1 [40320/60000]\tLoss: 0.262699\n",
            "Train Epoch: 1 [40960/60000]\tLoss: 0.322415\n",
            "Train Epoch: 1 [41600/60000]\tLoss: 0.148959\n",
            "Train Epoch: 1 [42240/60000]\tLoss: 0.282626\n",
            "Train Epoch: 1 [42880/60000]\tLoss: 0.298362\n",
            "Train Epoch: 1 [43520/60000]\tLoss: 0.295397\n",
            "Train Epoch: 1 [44160/60000]\tLoss: 0.258590\n",
            "Train Epoch: 1 [44800/60000]\tLoss: 0.404282\n",
            "Train Epoch: 1 [45440/60000]\tLoss: 0.218092\n",
            "Train Epoch: 1 [46080/60000]\tLoss: 0.187288\n",
            "Train Epoch: 1 [46720/60000]\tLoss: 0.280107\n",
            "Train Epoch: 1 [47360/60000]\tLoss: 0.323282\n",
            "Train Epoch: 1 [48000/60000]\tLoss: 0.376565\n",
            "Train Epoch: 1 [48640/60000]\tLoss: 0.415454\n",
            "Train Epoch: 1 [49280/60000]\tLoss: 0.209387\n",
            "Train Epoch: 1 [49920/60000]\tLoss: 0.400600\n",
            "Train Epoch: 1 [50560/60000]\tLoss: 0.265190\n",
            "Train Epoch: 1 [51200/60000]\tLoss: 0.436449\n",
            "Train Epoch: 1 [51840/60000]\tLoss: 0.210038\n",
            "Train Epoch: 1 [52480/60000]\tLoss: 0.242914\n",
            "Train Epoch: 1 [53120/60000]\tLoss: 0.167429\n",
            "Train Epoch: 1 [53760/60000]\tLoss: 0.275579\n",
            "Train Epoch: 1 [54400/60000]\tLoss: 0.150037\n",
            "Train Epoch: 1 [55040/60000]\tLoss: 0.226538\n",
            "Train Epoch: 1 [55680/60000]\tLoss: 0.388622\n",
            "Train Epoch: 1 [56320/60000]\tLoss: 0.297745\n",
            "Train Epoch: 1 [56960/60000]\tLoss: 0.168191\n",
            "Train Epoch: 1 [57600/60000]\tLoss: 0.266774\n",
            "Train Epoch: 1 [58240/60000]\tLoss: 0.097520\n",
            "Train Epoch: 1 [58880/60000]\tLoss: 0.236245\n",
            "Train Epoch: 1 [59520/60000]\tLoss: 0.232817\n",
            "Train Epoch: 2 [0/60000]\tLoss: 0.086222\n",
            "Train Epoch: 2 [640/60000]\tLoss: 0.300127\n",
            "Train Epoch: 2 [1280/60000]\tLoss: 0.216543\n",
            "Train Epoch: 2 [1920/60000]\tLoss: 0.409796\n",
            "Train Epoch: 2 [2560/60000]\tLoss: 0.165112\n",
            "Train Epoch: 2 [3200/60000]\tLoss: 0.114113\n",
            "Train Epoch: 2 [3840/60000]\tLoss: 0.179784\n",
            "Train Epoch: 2 [4480/60000]\tLoss: 0.159455\n",
            "Train Epoch: 2 [5120/60000]\tLoss: 0.200375\n",
            "Train Epoch: 2 [5760/60000]\tLoss: 0.147413\n",
            "Train Epoch: 2 [6400/60000]\tLoss: 0.263404\n",
            "Train Epoch: 2 [7040/60000]\tLoss: 0.307850\n",
            "Train Epoch: 2 [7680/60000]\tLoss: 0.298654\n",
            "Train Epoch: 2 [8320/60000]\tLoss: 0.134393\n",
            "Train Epoch: 2 [8960/60000]\tLoss: 0.356538\n",
            "Train Epoch: 2 [9600/60000]\tLoss: 0.311222\n",
            "Train Epoch: 2 [10240/60000]\tLoss: 0.211344\n",
            "Train Epoch: 2 [10880/60000]\tLoss: 0.208160\n",
            "Train Epoch: 2 [11520/60000]\tLoss: 0.228050\n",
            "Train Epoch: 2 [12160/60000]\tLoss: 0.172623\n",
            "Train Epoch: 2 [12800/60000]\tLoss: 0.302291\n",
            "Train Epoch: 2 [13440/60000]\tLoss: 0.181286\n",
            "Train Epoch: 2 [14080/60000]\tLoss: 0.114098\n",
            "Train Epoch: 2 [14720/60000]\tLoss: 0.164184\n",
            "Train Epoch: 2 [15360/60000]\tLoss: 0.165248\n",
            "Train Epoch: 2 [16000/60000]\tLoss: 0.148139\n",
            "Train Epoch: 2 [16640/60000]\tLoss: 0.344015\n",
            "Train Epoch: 2 [17280/60000]\tLoss: 0.075584\n",
            "Train Epoch: 2 [17920/60000]\tLoss: 0.188821\n",
            "Train Epoch: 2 [18560/60000]\tLoss: 0.317965\n",
            "Train Epoch: 2 [19200/60000]\tLoss: 0.200449\n",
            "Train Epoch: 2 [19840/60000]\tLoss: 0.167243\n",
            "Train Epoch: 2 [20480/60000]\tLoss: 0.176080\n",
            "Train Epoch: 2 [21120/60000]\tLoss: 0.199072\n",
            "Train Epoch: 2 [21760/60000]\tLoss: 0.217167\n",
            "Train Epoch: 2 [22400/60000]\tLoss: 0.100819\n",
            "Train Epoch: 2 [23040/60000]\tLoss: 0.193417\n",
            "Train Epoch: 2 [23680/60000]\tLoss: 0.201315\n",
            "Train Epoch: 2 [24320/60000]\tLoss: 0.244374\n",
            "Train Epoch: 2 [24960/60000]\tLoss: 0.178766\n",
            "Train Epoch: 2 [25600/60000]\tLoss: 0.181614\n",
            "Train Epoch: 2 [26240/60000]\tLoss: 0.196927\n",
            "Train Epoch: 2 [26880/60000]\tLoss: 0.222852\n",
            "Train Epoch: 2 [27520/60000]\tLoss: 0.062586\n",
            "Train Epoch: 2 [28160/60000]\tLoss: 0.188719\n",
            "Train Epoch: 2 [28800/60000]\tLoss: 0.230993\n",
            "Train Epoch: 2 [29440/60000]\tLoss: 0.201421\n",
            "Train Epoch: 2 [30080/60000]\tLoss: 0.174261\n",
            "Train Epoch: 2 [30720/60000]\tLoss: 0.177132\n",
            "Train Epoch: 2 [31360/60000]\tLoss: 0.108813\n",
            "Train Epoch: 2 [32000/60000]\tLoss: 0.246561\n",
            "Train Epoch: 2 [32640/60000]\tLoss: 0.080706\n",
            "Train Epoch: 2 [33280/60000]\tLoss: 0.193171\n",
            "Train Epoch: 2 [33920/60000]\tLoss: 0.261086\n",
            "Train Epoch: 2 [34560/60000]\tLoss: 0.226635\n",
            "Train Epoch: 2 [35200/60000]\tLoss: 0.092631\n",
            "Train Epoch: 2 [35840/60000]\tLoss: 0.145249\n",
            "Train Epoch: 2 [36480/60000]\tLoss: 0.227357\n",
            "Train Epoch: 2 [37120/60000]\tLoss: 0.168265\n",
            "Train Epoch: 2 [37760/60000]\tLoss: 0.318642\n",
            "Train Epoch: 2 [38400/60000]\tLoss: 0.168710\n",
            "Train Epoch: 2 [39040/60000]\tLoss: 0.182350\n",
            "Train Epoch: 2 [39680/60000]\tLoss: 0.190806\n",
            "Train Epoch: 2 [40320/60000]\tLoss: 0.136196\n",
            "Train Epoch: 2 [40960/60000]\tLoss: 0.082287\n",
            "Train Epoch: 2 [41600/60000]\tLoss: 0.136624\n",
            "Train Epoch: 2 [42240/60000]\tLoss: 0.290940\n",
            "Train Epoch: 2 [42880/60000]\tLoss: 0.103668\n",
            "Train Epoch: 2 [43520/60000]\tLoss: 0.092620\n",
            "Train Epoch: 2 [44160/60000]\tLoss: 0.147249\n",
            "Train Epoch: 2 [44800/60000]\tLoss: 0.069033\n",
            "Train Epoch: 2 [45440/60000]\tLoss: 0.363823\n",
            "Train Epoch: 2 [46080/60000]\tLoss: 0.109745\n",
            "Train Epoch: 2 [46720/60000]\tLoss: 0.163993\n",
            "Train Epoch: 2 [47360/60000]\tLoss: 0.170078\n",
            "Train Epoch: 2 [48000/60000]\tLoss: 0.212616\n",
            "Train Epoch: 2 [48640/60000]\tLoss: 0.082902\n",
            "Train Epoch: 2 [49280/60000]\tLoss: 0.161470\n",
            "Train Epoch: 2 [49920/60000]\tLoss: 0.059200\n",
            "Train Epoch: 2 [50560/60000]\tLoss: 0.184532\n",
            "Train Epoch: 2 [51200/60000]\tLoss: 0.265126\n",
            "Train Epoch: 2 [51840/60000]\tLoss: 0.097416\n",
            "Train Epoch: 2 [52480/60000]\tLoss: 0.161638\n",
            "Train Epoch: 2 [53120/60000]\tLoss: 0.185423\n",
            "Train Epoch: 2 [53760/60000]\tLoss: 0.030639\n",
            "Train Epoch: 2 [54400/60000]\tLoss: 0.404948\n",
            "Train Epoch: 2 [55040/60000]\tLoss: 0.159399\n",
            "Train Epoch: 2 [55680/60000]\tLoss: 0.164105\n",
            "Train Epoch: 2 [56320/60000]\tLoss: 0.096447\n",
            "Train Epoch: 2 [56960/60000]\tLoss: 0.148095\n",
            "Train Epoch: 2 [57600/60000]\tLoss: 0.245926\n",
            "Train Epoch: 2 [58240/60000]\tLoss: 0.403661\n",
            "Train Epoch: 2 [58880/60000]\tLoss: 0.116311\n",
            "Train Epoch: 2 [59520/60000]\tLoss: 0.206124\n",
            "Train Epoch: 3 [0/60000]\tLoss: 0.251866\n",
            "Train Epoch: 3 [640/60000]\tLoss: 0.182531\n",
            "Train Epoch: 3 [1280/60000]\tLoss: 0.128002\n",
            "Train Epoch: 3 [1920/60000]\tLoss: 0.175783\n",
            "Train Epoch: 3 [2560/60000]\tLoss: 0.127899\n",
            "Train Epoch: 3 [3200/60000]\tLoss: 0.118496\n",
            "Train Epoch: 3 [3840/60000]\tLoss: 0.110282\n",
            "Train Epoch: 3 [4480/60000]\tLoss: 0.229754\n",
            "Train Epoch: 3 [5120/60000]\tLoss: 0.088041\n",
            "Train Epoch: 3 [5760/60000]\tLoss: 0.265959\n",
            "Train Epoch: 3 [6400/60000]\tLoss: 0.213065\n",
            "Train Epoch: 3 [7040/60000]\tLoss: 0.148868\n",
            "Train Epoch: 3 [7680/60000]\tLoss: 0.043074\n",
            "Train Epoch: 3 [8320/60000]\tLoss: 0.306688\n",
            "Train Epoch: 3 [8960/60000]\tLoss: 0.125667\n",
            "Train Epoch: 3 [9600/60000]\tLoss: 0.175415\n",
            "Train Epoch: 3 [10240/60000]\tLoss: 0.090482\n",
            "Train Epoch: 3 [10880/60000]\tLoss: 0.139491\n",
            "Train Epoch: 3 [11520/60000]\tLoss: 0.178061\n",
            "Train Epoch: 3 [12160/60000]\tLoss: 0.030106\n",
            "Train Epoch: 3 [12800/60000]\tLoss: 0.169776\n",
            "Train Epoch: 3 [13440/60000]\tLoss: 0.128538\n",
            "Train Epoch: 3 [14080/60000]\tLoss: 0.280392\n",
            "Train Epoch: 3 [14720/60000]\tLoss: 0.111165\n",
            "Train Epoch: 3 [15360/60000]\tLoss: 0.104402\n",
            "Train Epoch: 3 [16000/60000]\tLoss: 0.198714\n",
            "Train Epoch: 3 [16640/60000]\tLoss: 0.117176\n",
            "Train Epoch: 3 [17280/60000]\tLoss: 0.156598\n",
            "Train Epoch: 3 [17920/60000]\tLoss: 0.108057\n",
            "Train Epoch: 3 [18560/60000]\tLoss: 0.051865\n",
            "Train Epoch: 3 [19200/60000]\tLoss: 0.121549\n",
            "Train Epoch: 3 [19840/60000]\tLoss: 0.114713\n",
            "Train Epoch: 3 [20480/60000]\tLoss: 0.097326\n",
            "Train Epoch: 3 [21120/60000]\tLoss: 0.055854\n",
            "Train Epoch: 3 [21760/60000]\tLoss: 0.200313\n",
            "Train Epoch: 3 [22400/60000]\tLoss: 0.119152\n",
            "Train Epoch: 3 [23040/60000]\tLoss: 0.283443\n",
            "Train Epoch: 3 [23680/60000]\tLoss: 0.178973\n",
            "Train Epoch: 3 [24320/60000]\tLoss: 0.131838\n",
            "Train Epoch: 3 [24960/60000]\tLoss: 0.097084\n",
            "Train Epoch: 3 [25600/60000]\tLoss: 0.290498\n",
            "Train Epoch: 3 [26240/60000]\tLoss: 0.408748\n",
            "Train Epoch: 3 [26880/60000]\tLoss: 0.147667\n",
            "Train Epoch: 3 [27520/60000]\tLoss: 0.096909\n",
            "Train Epoch: 3 [28160/60000]\tLoss: 0.079559\n",
            "Train Epoch: 3 [28800/60000]\tLoss: 0.056897\n",
            "Train Epoch: 3 [29440/60000]\tLoss: 0.174864\n",
            "Train Epoch: 3 [30080/60000]\tLoss: 0.222518\n",
            "Train Epoch: 3 [30720/60000]\tLoss: 0.124179\n",
            "Train Epoch: 3 [31360/60000]\tLoss: 0.219412\n",
            "Train Epoch: 3 [32000/60000]\tLoss: 0.141785\n",
            "Train Epoch: 3 [32640/60000]\tLoss: 0.223288\n",
            "Train Epoch: 3 [33280/60000]\tLoss: 0.232074\n",
            "Train Epoch: 3 [33920/60000]\tLoss: 0.216423\n",
            "Train Epoch: 3 [34560/60000]\tLoss: 0.158657\n",
            "Train Epoch: 3 [35200/60000]\tLoss: 0.102973\n",
            "Train Epoch: 3 [35840/60000]\tLoss: 0.146622\n",
            "Train Epoch: 3 [36480/60000]\tLoss: 0.210646\n",
            "Train Epoch: 3 [37120/60000]\tLoss: 0.321401\n",
            "Train Epoch: 3 [37760/60000]\tLoss: 0.094998\n",
            "Train Epoch: 3 [38400/60000]\tLoss: 0.090442\n",
            "Train Epoch: 3 [39040/60000]\tLoss: 0.062822\n",
            "Train Epoch: 3 [39680/60000]\tLoss: 0.163834\n",
            "Train Epoch: 3 [40320/60000]\tLoss: 0.074711\n",
            "Train Epoch: 3 [40960/60000]\tLoss: 0.027344\n",
            "Train Epoch: 3 [41600/60000]\tLoss: 0.046175\n",
            "Train Epoch: 3 [42240/60000]\tLoss: 0.093242\n",
            "Train Epoch: 3 [42880/60000]\tLoss: 0.160413\n",
            "Train Epoch: 3 [43520/60000]\tLoss: 0.110833\n",
            "Train Epoch: 3 [44160/60000]\tLoss: 0.083789\n",
            "Train Epoch: 3 [44800/60000]\tLoss: 0.054880\n",
            "Train Epoch: 3 [45440/60000]\tLoss: 0.191592\n",
            "Train Epoch: 3 [46080/60000]\tLoss: 0.195350\n",
            "Train Epoch: 3 [46720/60000]\tLoss: 0.044601\n",
            "Train Epoch: 3 [47360/60000]\tLoss: 0.137056\n",
            "Train Epoch: 3 [48000/60000]\tLoss: 0.126141\n",
            "Train Epoch: 3 [48640/60000]\tLoss: 0.258504\n",
            "Train Epoch: 3 [49280/60000]\tLoss: 0.097569\n",
            "Train Epoch: 3 [49920/60000]\tLoss: 0.104359\n",
            "Train Epoch: 3 [50560/60000]\tLoss: 0.132122\n",
            "Train Epoch: 3 [51200/60000]\tLoss: 0.179478\n",
            "Train Epoch: 3 [51840/60000]\tLoss: 0.161342\n",
            "Train Epoch: 3 [52480/60000]\tLoss: 0.048446\n",
            "Train Epoch: 3 [53120/60000]\tLoss: 0.093116\n",
            "Train Epoch: 3 [53760/60000]\tLoss: 0.083759\n",
            "Train Epoch: 3 [54400/60000]\tLoss: 0.226155\n",
            "Train Epoch: 3 [55040/60000]\tLoss: 0.084661\n",
            "Train Epoch: 3 [55680/60000]\tLoss: 0.177711\n",
            "Train Epoch: 3 [56320/60000]\tLoss: 0.153694\n",
            "Train Epoch: 3 [56960/60000]\tLoss: 0.240489\n",
            "Train Epoch: 3 [57600/60000]\tLoss: 0.095387\n",
            "Train Epoch: 3 [58240/60000]\tLoss: 0.147509\n",
            "Train Epoch: 3 [58880/60000]\tLoss: 0.155260\n",
            "Train Epoch: 3 [59520/60000]\tLoss: 0.202257\n",
            "Train Epoch: 4 [0/60000]\tLoss: 0.176203\n",
            "Train Epoch: 4 [640/60000]\tLoss: 0.252328\n",
            "Train Epoch: 4 [1280/60000]\tLoss: 0.163351\n",
            "Train Epoch: 4 [1920/60000]\tLoss: 0.057038\n",
            "Train Epoch: 4 [2560/60000]\tLoss: 0.094104\n",
            "Train Epoch: 4 [3200/60000]\tLoss: 0.068423\n",
            "Train Epoch: 4 [3840/60000]\tLoss: 0.040918\n",
            "Train Epoch: 4 [4480/60000]\tLoss: 0.095688\n",
            "Train Epoch: 4 [5120/60000]\tLoss: 0.228377\n",
            "Train Epoch: 4 [5760/60000]\tLoss: 0.107730\n",
            "Train Epoch: 4 [6400/60000]\tLoss: 0.274287\n",
            "Train Epoch: 4 [7040/60000]\tLoss: 0.096030\n",
            "Train Epoch: 4 [7680/60000]\tLoss: 0.116263\n",
            "Train Epoch: 4 [8320/60000]\tLoss: 0.065388\n",
            "Train Epoch: 4 [8960/60000]\tLoss: 0.140133\n",
            "Train Epoch: 4 [9600/60000]\tLoss: 0.165494\n",
            "Train Epoch: 4 [10240/60000]\tLoss: 0.107532\n",
            "Train Epoch: 4 [10880/60000]\tLoss: 0.155798\n",
            "Train Epoch: 4 [11520/60000]\tLoss: 0.180679\n",
            "Train Epoch: 4 [12160/60000]\tLoss: 0.089248\n",
            "Train Epoch: 4 [12800/60000]\tLoss: 0.149384\n",
            "Train Epoch: 4 [13440/60000]\tLoss: 0.146947\n",
            "Train Epoch: 4 [14080/60000]\tLoss: 0.129480\n",
            "Train Epoch: 4 [14720/60000]\tLoss: 0.144732\n",
            "Train Epoch: 4 [15360/60000]\tLoss: 0.358504\n",
            "Train Epoch: 4 [16000/60000]\tLoss: 0.129514\n",
            "Train Epoch: 4 [16640/60000]\tLoss: 0.170257\n",
            "Train Epoch: 4 [17280/60000]\tLoss: 0.111554\n",
            "Train Epoch: 4 [17920/60000]\tLoss: 0.052657\n",
            "Train Epoch: 4 [18560/60000]\tLoss: 0.070753\n",
            "Train Epoch: 4 [19200/60000]\tLoss: 0.214033\n",
            "Train Epoch: 4 [19840/60000]\tLoss: 0.176185\n",
            "Train Epoch: 4 [20480/60000]\tLoss: 0.188731\n",
            "Train Epoch: 4 [21120/60000]\tLoss: 0.028440\n",
            "Train Epoch: 4 [21760/60000]\tLoss: 0.082240\n",
            "Train Epoch: 4 [22400/60000]\tLoss: 0.071375\n",
            "Train Epoch: 4 [23040/60000]\tLoss: 0.208130\n",
            "Train Epoch: 4 [23680/60000]\tLoss: 0.145747\n",
            "Train Epoch: 4 [24320/60000]\tLoss: 0.093125\n",
            "Train Epoch: 4 [24960/60000]\tLoss: 0.100546\n",
            "Train Epoch: 4 [25600/60000]\tLoss: 0.254601\n",
            "Train Epoch: 4 [26240/60000]\tLoss: 0.093740\n",
            "Train Epoch: 4 [26880/60000]\tLoss: 0.129342\n",
            "Train Epoch: 4 [27520/60000]\tLoss: 0.307157\n",
            "Train Epoch: 4 [28160/60000]\tLoss: 0.060393\n",
            "Train Epoch: 4 [28800/60000]\tLoss: 0.213833\n",
            "Train Epoch: 4 [29440/60000]\tLoss: 0.118438\n",
            "Train Epoch: 4 [30080/60000]\tLoss: 0.080153\n",
            "Train Epoch: 4 [30720/60000]\tLoss: 0.082120\n",
            "Train Epoch: 4 [31360/60000]\tLoss: 0.059487\n",
            "Train Epoch: 4 [32000/60000]\tLoss: 0.109582\n",
            "Train Epoch: 4 [32640/60000]\tLoss: 0.282545\n",
            "Train Epoch: 4 [33280/60000]\tLoss: 0.064069\n",
            "Train Epoch: 4 [33920/60000]\tLoss: 0.091667\n",
            "Train Epoch: 4 [34560/60000]\tLoss: 0.155283\n",
            "Train Epoch: 4 [35200/60000]\tLoss: 0.108457\n",
            "Train Epoch: 4 [35840/60000]\tLoss: 0.054404\n",
            "Train Epoch: 4 [36480/60000]\tLoss: 0.252202\n",
            "Train Epoch: 4 [37120/60000]\tLoss: 0.218818\n",
            "Train Epoch: 4 [37760/60000]\tLoss: 0.268798\n",
            "Train Epoch: 4 [38400/60000]\tLoss: 0.046966\n",
            "Train Epoch: 4 [39040/60000]\tLoss: 0.061786\n",
            "Train Epoch: 4 [39680/60000]\tLoss: 0.180429\n",
            "Train Epoch: 4 [40320/60000]\tLoss: 0.118281\n",
            "Train Epoch: 4 [40960/60000]\tLoss: 0.135767\n",
            "Train Epoch: 4 [41600/60000]\tLoss: 0.080065\n",
            "Train Epoch: 4 [42240/60000]\tLoss: 0.134528\n",
            "Train Epoch: 4 [42880/60000]\tLoss: 0.174149\n",
            "Train Epoch: 4 [43520/60000]\tLoss: 0.055342\n",
            "Train Epoch: 4 [44160/60000]\tLoss: 0.130770\n",
            "Train Epoch: 4 [44800/60000]\tLoss: 0.144611\n",
            "Train Epoch: 4 [45440/60000]\tLoss: 0.114791\n",
            "Train Epoch: 4 [46080/60000]\tLoss: 0.130145\n",
            "Train Epoch: 4 [46720/60000]\tLoss: 0.078107\n",
            "Train Epoch: 4 [47360/60000]\tLoss: 0.225699\n",
            "Train Epoch: 4 [48000/60000]\tLoss: 0.133354\n",
            "Train Epoch: 4 [48640/60000]\tLoss: 0.211189\n",
            "Train Epoch: 4 [49280/60000]\tLoss: 0.064729\n",
            "Train Epoch: 4 [49920/60000]\tLoss: 0.060091\n",
            "Train Epoch: 4 [50560/60000]\tLoss: 0.061786\n",
            "Train Epoch: 4 [51200/60000]\tLoss: 0.011251\n",
            "Train Epoch: 4 [51840/60000]\tLoss: 0.085545\n",
            "Train Epoch: 4 [52480/60000]\tLoss: 0.048593\n",
            "Train Epoch: 4 [53120/60000]\tLoss: 0.259831\n",
            "Train Epoch: 4 [53760/60000]\tLoss: 0.182683\n",
            "Train Epoch: 4 [54400/60000]\tLoss: 0.102234\n",
            "Train Epoch: 4 [55040/60000]\tLoss: 0.081487\n",
            "Train Epoch: 4 [55680/60000]\tLoss: 0.178805\n",
            "Train Epoch: 4 [56320/60000]\tLoss: 0.076821\n",
            "Train Epoch: 4 [56960/60000]\tLoss: 0.277776\n",
            "Train Epoch: 4 [57600/60000]\tLoss: 0.230712\n",
            "Train Epoch: 4 [58240/60000]\tLoss: 0.080906\n",
            "Train Epoch: 4 [58880/60000]\tLoss: 0.112121\n",
            "Train Epoch: 4 [59520/60000]\tLoss: 0.113928\n",
            "Train Epoch: 5 [0/60000]\tLoss: 0.026382\n",
            "Train Epoch: 5 [640/60000]\tLoss: 0.111161\n",
            "Train Epoch: 5 [1280/60000]\tLoss: 0.128214\n",
            "Train Epoch: 5 [1920/60000]\tLoss: 0.067516\n",
            "Train Epoch: 5 [2560/60000]\tLoss: 0.071722\n",
            "Train Epoch: 5 [3200/60000]\tLoss: 0.084443\n",
            "Train Epoch: 5 [3840/60000]\tLoss: 0.163509\n",
            "Train Epoch: 5 [4480/60000]\tLoss: 0.079266\n",
            "Train Epoch: 5 [5120/60000]\tLoss: 0.075710\n",
            "Train Epoch: 5 [5760/60000]\tLoss: 0.078732\n",
            "Train Epoch: 5 [6400/60000]\tLoss: 0.143154\n",
            "Train Epoch: 5 [7040/60000]\tLoss: 0.243562\n",
            "Train Epoch: 5 [7680/60000]\tLoss: 0.159497\n",
            "Train Epoch: 5 [8320/60000]\tLoss: 0.047036\n",
            "Train Epoch: 5 [8960/60000]\tLoss: 0.102769\n",
            "Train Epoch: 5 [9600/60000]\tLoss: 0.237820\n",
            "Train Epoch: 5 [10240/60000]\tLoss: 0.117764\n",
            "Train Epoch: 5 [10880/60000]\tLoss: 0.091518\n",
            "Train Epoch: 5 [11520/60000]\tLoss: 0.116110\n",
            "Train Epoch: 5 [12160/60000]\tLoss: 0.066728\n",
            "Train Epoch: 5 [12800/60000]\tLoss: 0.261782\n",
            "Train Epoch: 5 [13440/60000]\tLoss: 0.143867\n",
            "Train Epoch: 5 [14080/60000]\tLoss: 0.262385\n",
            "Train Epoch: 5 [14720/60000]\tLoss: 0.129239\n",
            "Train Epoch: 5 [15360/60000]\tLoss: 0.065407\n",
            "Train Epoch: 5 [16000/60000]\tLoss: 0.112092\n",
            "Train Epoch: 5 [16640/60000]\tLoss: 0.115127\n",
            "Train Epoch: 5 [17280/60000]\tLoss: 0.213753\n",
            "Train Epoch: 5 [17920/60000]\tLoss: 0.154744\n",
            "Train Epoch: 5 [18560/60000]\tLoss: 0.117344\n",
            "Train Epoch: 5 [19200/60000]\tLoss: 0.181971\n",
            "Train Epoch: 5 [19840/60000]\tLoss: 0.199127\n",
            "Train Epoch: 5 [20480/60000]\tLoss: 0.075965\n",
            "Train Epoch: 5 [21120/60000]\tLoss: 0.024590\n",
            "Train Epoch: 5 [21760/60000]\tLoss: 0.289327\n",
            "Train Epoch: 5 [22400/60000]\tLoss: 0.072618\n",
            "Train Epoch: 5 [23040/60000]\tLoss: 0.159695\n",
            "Train Epoch: 5 [23680/60000]\tLoss: 0.179106\n",
            "Train Epoch: 5 [24320/60000]\tLoss: 0.025202\n",
            "Train Epoch: 5 [24960/60000]\tLoss: 0.059293\n",
            "Train Epoch: 5 [25600/60000]\tLoss: 0.074213\n",
            "Train Epoch: 5 [26240/60000]\tLoss: 0.041729\n",
            "Train Epoch: 5 [26880/60000]\tLoss: 0.182752\n",
            "Train Epoch: 5 [27520/60000]\tLoss: 0.112727\n",
            "Train Epoch: 5 [28160/60000]\tLoss: 0.082460\n",
            "Train Epoch: 5 [28800/60000]\tLoss: 0.109819\n",
            "Train Epoch: 5 [29440/60000]\tLoss: 0.051663\n",
            "Train Epoch: 5 [30080/60000]\tLoss: 0.142051\n",
            "Train Epoch: 5 [30720/60000]\tLoss: 0.038056\n",
            "Train Epoch: 5 [31360/60000]\tLoss: 0.086127\n",
            "Train Epoch: 5 [32000/60000]\tLoss: 0.184887\n",
            "Train Epoch: 5 [32640/60000]\tLoss: 0.130481\n",
            "Train Epoch: 5 [33280/60000]\tLoss: 0.241162\n",
            "Train Epoch: 5 [33920/60000]\tLoss: 0.218290\n",
            "Train Epoch: 5 [34560/60000]\tLoss: 0.195998\n",
            "Train Epoch: 5 [35200/60000]\tLoss: 0.331723\n",
            "Train Epoch: 5 [35840/60000]\tLoss: 0.113944\n",
            "Train Epoch: 5 [36480/60000]\tLoss: 0.065444\n",
            "Train Epoch: 5 [37120/60000]\tLoss: 0.154789\n",
            "Train Epoch: 5 [37760/60000]\tLoss: 0.053180\n",
            "Train Epoch: 5 [38400/60000]\tLoss: 0.016999\n",
            "Train Epoch: 5 [39040/60000]\tLoss: 0.155541\n",
            "Train Epoch: 5 [39680/60000]\tLoss: 0.204985\n",
            "Train Epoch: 5 [40320/60000]\tLoss: 0.032765\n",
            "Train Epoch: 5 [40960/60000]\tLoss: 0.082453\n",
            "Train Epoch: 5 [41600/60000]\tLoss: 0.072220\n",
            "Train Epoch: 5 [42240/60000]\tLoss: 0.095894\n",
            "Train Epoch: 5 [42880/60000]\tLoss: 0.162250\n",
            "Train Epoch: 5 [43520/60000]\tLoss: 0.039040\n",
            "Train Epoch: 5 [44160/60000]\tLoss: 0.097592\n",
            "Train Epoch: 5 [44800/60000]\tLoss: 0.019107\n",
            "Train Epoch: 5 [45440/60000]\tLoss: 0.062945\n",
            "Train Epoch: 5 [46080/60000]\tLoss: 0.272293\n",
            "Train Epoch: 5 [46720/60000]\tLoss: 0.054910\n",
            "Train Epoch: 5 [47360/60000]\tLoss: 0.157649\n",
            "Train Epoch: 5 [48000/60000]\tLoss: 0.173694\n",
            "Train Epoch: 5 [48640/60000]\tLoss: 0.081861\n",
            "Train Epoch: 5 [49280/60000]\tLoss: 0.056536\n",
            "Train Epoch: 5 [49920/60000]\tLoss: 0.130664\n",
            "Train Epoch: 5 [50560/60000]\tLoss: 0.237415\n",
            "Train Epoch: 5 [51200/60000]\tLoss: 0.041501\n",
            "Train Epoch: 5 [51840/60000]\tLoss: 0.170072\n",
            "Train Epoch: 5 [52480/60000]\tLoss: 0.146447\n",
            "Train Epoch: 5 [53120/60000]\tLoss: 0.043629\n",
            "Train Epoch: 5 [53760/60000]\tLoss: 0.183961\n",
            "Train Epoch: 5 [54400/60000]\tLoss: 0.042685\n",
            "Train Epoch: 5 [55040/60000]\tLoss: 0.072917\n",
            "Train Epoch: 5 [55680/60000]\tLoss: 0.048253\n",
            "Train Epoch: 5 [56320/60000]\tLoss: 0.179666\n",
            "Train Epoch: 5 [56960/60000]\tLoss: 0.064996\n",
            "Train Epoch: 5 [57600/60000]\tLoss: 0.125394\n",
            "Train Epoch: 5 [58240/60000]\tLoss: 0.032435\n",
            "Train Epoch: 5 [58880/60000]\tLoss: 0.122433\n",
            "Train Epoch: 5 [59520/60000]\tLoss: 0.113122\n",
            "Train Epoch: 6 [0/60000]\tLoss: 0.040477\n",
            "Train Epoch: 6 [640/60000]\tLoss: 0.057508\n",
            "Train Epoch: 6 [1280/60000]\tLoss: 0.251686\n",
            "Train Epoch: 6 [1920/60000]\tLoss: 0.192765\n",
            "Train Epoch: 6 [2560/60000]\tLoss: 0.092167\n",
            "Train Epoch: 6 [3200/60000]\tLoss: 0.324564\n",
            "Train Epoch: 6 [3840/60000]\tLoss: 0.194086\n",
            "Train Epoch: 6 [4480/60000]\tLoss: 0.118608\n",
            "Train Epoch: 6 [5120/60000]\tLoss: 0.114124\n",
            "Train Epoch: 6 [5760/60000]\tLoss: 0.065915\n",
            "Train Epoch: 6 [6400/60000]\tLoss: 0.049433\n",
            "Train Epoch: 6 [7040/60000]\tLoss: 0.075932\n",
            "Train Epoch: 6 [7680/60000]\tLoss: 0.064780\n",
            "Train Epoch: 6 [8320/60000]\tLoss: 0.074518\n",
            "Train Epoch: 6 [8960/60000]\tLoss: 0.131189\n",
            "Train Epoch: 6 [9600/60000]\tLoss: 0.030436\n",
            "Train Epoch: 6 [10240/60000]\tLoss: 0.166134\n",
            "Train Epoch: 6 [10880/60000]\tLoss: 0.167136\n",
            "Train Epoch: 6 [11520/60000]\tLoss: 0.044515\n",
            "Train Epoch: 6 [12160/60000]\tLoss: 0.028264\n",
            "Train Epoch: 6 [12800/60000]\tLoss: 0.057607\n",
            "Train Epoch: 6 [13440/60000]\tLoss: 0.038745\n",
            "Train Epoch: 6 [14080/60000]\tLoss: 0.096178\n",
            "Train Epoch: 6 [14720/60000]\tLoss: 0.237733\n",
            "Train Epoch: 6 [15360/60000]\tLoss: 0.016365\n",
            "Train Epoch: 6 [16000/60000]\tLoss: 0.080193\n",
            "Train Epoch: 6 [16640/60000]\tLoss: 0.073683\n",
            "Train Epoch: 6 [17280/60000]\tLoss: 0.030070\n",
            "Train Epoch: 6 [17920/60000]\tLoss: 0.043701\n",
            "Train Epoch: 6 [18560/60000]\tLoss: 0.110769\n",
            "Train Epoch: 6 [19200/60000]\tLoss: 0.083626\n",
            "Train Epoch: 6 [19840/60000]\tLoss: 0.029801\n",
            "Train Epoch: 6 [20480/60000]\tLoss: 0.012748\n",
            "Train Epoch: 6 [21120/60000]\tLoss: 0.033770\n",
            "Train Epoch: 6 [21760/60000]\tLoss: 0.107823\n",
            "Train Epoch: 6 [22400/60000]\tLoss: 0.113856\n",
            "Train Epoch: 6 [23040/60000]\tLoss: 0.101856\n",
            "Train Epoch: 6 [23680/60000]\tLoss: 0.046841\n",
            "Train Epoch: 6 [24320/60000]\tLoss: 0.270572\n",
            "Train Epoch: 6 [24960/60000]\tLoss: 0.073930\n",
            "Train Epoch: 6 [25600/60000]\tLoss: 0.044390\n",
            "Train Epoch: 6 [26240/60000]\tLoss: 0.169676\n",
            "Train Epoch: 6 [26880/60000]\tLoss: 0.028539\n",
            "Train Epoch: 6 [27520/60000]\tLoss: 0.119583\n",
            "Train Epoch: 6 [28160/60000]\tLoss: 0.049840\n",
            "Train Epoch: 6 [28800/60000]\tLoss: 0.016270\n",
            "Train Epoch: 6 [29440/60000]\tLoss: 0.071304\n",
            "Train Epoch: 6 [30080/60000]\tLoss: 0.130552\n",
            "Train Epoch: 6 [30720/60000]\tLoss: 0.088891\n",
            "Train Epoch: 6 [31360/60000]\tLoss: 0.014329\n",
            "Train Epoch: 6 [32000/60000]\tLoss: 0.263698\n",
            "Train Epoch: 6 [32640/60000]\tLoss: 0.056553\n",
            "Train Epoch: 6 [33280/60000]\tLoss: 0.192149\n",
            "Train Epoch: 6 [33920/60000]\tLoss: 0.117519\n",
            "Train Epoch: 6 [34560/60000]\tLoss: 0.164128\n",
            "Train Epoch: 6 [35200/60000]\tLoss: 0.173307\n",
            "Train Epoch: 6 [35840/60000]\tLoss: 0.219335\n",
            "Train Epoch: 6 [36480/60000]\tLoss: 0.117804\n",
            "Train Epoch: 6 [37120/60000]\tLoss: 0.148026\n",
            "Train Epoch: 6 [37760/60000]\tLoss: 0.049752\n",
            "Train Epoch: 6 [38400/60000]\tLoss: 0.071438\n",
            "Train Epoch: 6 [39040/60000]\tLoss: 0.109279\n",
            "Train Epoch: 6 [39680/60000]\tLoss: 0.095532\n",
            "Train Epoch: 6 [40320/60000]\tLoss: 0.050034\n",
            "Train Epoch: 6 [40960/60000]\tLoss: 0.194402\n",
            "Train Epoch: 6 [41600/60000]\tLoss: 0.029931\n",
            "Train Epoch: 6 [42240/60000]\tLoss: 0.036135\n",
            "Train Epoch: 6 [42880/60000]\tLoss: 0.086998\n",
            "Train Epoch: 6 [43520/60000]\tLoss: 0.164600\n",
            "Train Epoch: 6 [44160/60000]\tLoss: 0.101204\n",
            "Train Epoch: 6 [44800/60000]\tLoss: 0.050987\n",
            "Train Epoch: 6 [45440/60000]\tLoss: 0.081104\n",
            "Train Epoch: 6 [46080/60000]\tLoss: 0.283616\n",
            "Train Epoch: 6 [46720/60000]\tLoss: 0.068362\n",
            "Train Epoch: 6 [47360/60000]\tLoss: 0.222652\n",
            "Train Epoch: 6 [48000/60000]\tLoss: 0.114601\n",
            "Train Epoch: 6 [48640/60000]\tLoss: 0.146007\n",
            "Train Epoch: 6 [49280/60000]\tLoss: 0.190833\n",
            "Train Epoch: 6 [49920/60000]\tLoss: 0.226312\n",
            "Train Epoch: 6 [50560/60000]\tLoss: 0.086207\n",
            "Train Epoch: 6 [51200/60000]\tLoss: 0.135081\n",
            "Train Epoch: 6 [51840/60000]\tLoss: 0.054816\n",
            "Train Epoch: 6 [52480/60000]\tLoss: 0.116681\n",
            "Train Epoch: 6 [53120/60000]\tLoss: 0.082290\n",
            "Train Epoch: 6 [53760/60000]\tLoss: 0.040075\n",
            "Train Epoch: 6 [54400/60000]\tLoss: 0.063539\n",
            "Train Epoch: 6 [55040/60000]\tLoss: 0.032530\n",
            "Train Epoch: 6 [55680/60000]\tLoss: 0.042365\n",
            "Train Epoch: 6 [56320/60000]\tLoss: 0.170784\n",
            "Train Epoch: 6 [56960/60000]\tLoss: 0.191405\n",
            "Train Epoch: 6 [57600/60000]\tLoss: 0.082120\n",
            "Train Epoch: 6 [58240/60000]\tLoss: 0.035900\n",
            "Train Epoch: 6 [58880/60000]\tLoss: 0.132839\n",
            "Train Epoch: 6 [59520/60000]\tLoss: 0.130153\n",
            "Train Epoch: 7 [0/60000]\tLoss: 0.146508\n",
            "Train Epoch: 7 [640/60000]\tLoss: 0.051623\n",
            "Train Epoch: 7 [1280/60000]\tLoss: 0.051703\n",
            "Train Epoch: 7 [1920/60000]\tLoss: 0.187313\n",
            "Train Epoch: 7 [2560/60000]\tLoss: 0.095784\n",
            "Train Epoch: 7 [3200/60000]\tLoss: 0.198104\n",
            "Train Epoch: 7 [3840/60000]\tLoss: 0.126227\n",
            "Train Epoch: 7 [4480/60000]\tLoss: 0.077764\n",
            "Train Epoch: 7 [5120/60000]\tLoss: 0.066767\n",
            "Train Epoch: 7 [5760/60000]\tLoss: 0.092418\n",
            "Train Epoch: 7 [6400/60000]\tLoss: 0.088814\n",
            "Train Epoch: 7 [7040/60000]\tLoss: 0.186815\n",
            "Train Epoch: 7 [7680/60000]\tLoss: 0.047123\n",
            "Train Epoch: 7 [8320/60000]\tLoss: 0.079736\n",
            "Train Epoch: 7 [8960/60000]\tLoss: 0.055576\n",
            "Train Epoch: 7 [9600/60000]\tLoss: 0.050088\n",
            "Train Epoch: 7 [10240/60000]\tLoss: 0.143918\n",
            "Train Epoch: 7 [10880/60000]\tLoss: 0.139223\n",
            "Train Epoch: 7 [11520/60000]\tLoss: 0.034881\n",
            "Train Epoch: 7 [12160/60000]\tLoss: 0.122603\n",
            "Train Epoch: 7 [12800/60000]\tLoss: 0.178939\n",
            "Train Epoch: 7 [13440/60000]\tLoss: 0.044304\n",
            "Train Epoch: 7 [14080/60000]\tLoss: 0.115831\n",
            "Train Epoch: 7 [14720/60000]\tLoss: 0.098231\n",
            "Train Epoch: 7 [15360/60000]\tLoss: 0.049025\n",
            "Train Epoch: 7 [16000/60000]\tLoss: 0.145561\n",
            "Train Epoch: 7 [16640/60000]\tLoss: 0.290156\n",
            "Train Epoch: 7 [17280/60000]\tLoss: 0.154979\n",
            "Train Epoch: 7 [17920/60000]\tLoss: 0.027927\n",
            "Train Epoch: 7 [18560/60000]\tLoss: 0.070632\n",
            "Train Epoch: 7 [19200/60000]\tLoss: 0.103555\n",
            "Train Epoch: 7 [19840/60000]\tLoss: 0.169986\n",
            "Train Epoch: 7 [20480/60000]\tLoss: 0.199728\n",
            "Train Epoch: 7 [21120/60000]\tLoss: 0.078495\n",
            "Train Epoch: 7 [21760/60000]\tLoss: 0.020457\n",
            "Train Epoch: 7 [22400/60000]\tLoss: 0.032928\n",
            "Train Epoch: 7 [23040/60000]\tLoss: 0.051767\n",
            "Train Epoch: 7 [23680/60000]\tLoss: 0.071941\n",
            "Train Epoch: 7 [24320/60000]\tLoss: 0.116473\n",
            "Train Epoch: 7 [24960/60000]\tLoss: 0.112428\n",
            "Train Epoch: 7 [25600/60000]\tLoss: 0.127126\n",
            "Train Epoch: 7 [26240/60000]\tLoss: 0.098837\n",
            "Train Epoch: 7 [26880/60000]\tLoss: 0.251955\n",
            "Train Epoch: 7 [27520/60000]\tLoss: 0.125065\n",
            "Train Epoch: 7 [28160/60000]\tLoss: 0.090968\n",
            "Train Epoch: 7 [28800/60000]\tLoss: 0.173784\n",
            "Train Epoch: 7 [29440/60000]\tLoss: 0.024714\n",
            "Train Epoch: 7 [30080/60000]\tLoss: 0.039093\n",
            "Train Epoch: 7 [30720/60000]\tLoss: 0.045799\n",
            "Train Epoch: 7 [31360/60000]\tLoss: 0.376688\n",
            "Train Epoch: 7 [32000/60000]\tLoss: 0.125335\n",
            "Train Epoch: 7 [32640/60000]\tLoss: 0.068999\n",
            "Train Epoch: 7 [33280/60000]\tLoss: 0.082116\n",
            "Train Epoch: 7 [33920/60000]\tLoss: 0.013529\n",
            "Train Epoch: 7 [34560/60000]\tLoss: 0.067111\n",
            "Train Epoch: 7 [35200/60000]\tLoss: 0.070367\n",
            "Train Epoch: 7 [35840/60000]\tLoss: 0.147833\n",
            "Train Epoch: 7 [36480/60000]\tLoss: 0.063135\n",
            "Train Epoch: 7 [37120/60000]\tLoss: 0.055795\n",
            "Train Epoch: 7 [37760/60000]\tLoss: 0.049149\n",
            "Train Epoch: 7 [38400/60000]\tLoss: 0.010272\n",
            "Train Epoch: 7 [39040/60000]\tLoss: 0.093986\n",
            "Train Epoch: 7 [39680/60000]\tLoss: 0.048646\n",
            "Train Epoch: 7 [40320/60000]\tLoss: 0.128388\n",
            "Train Epoch: 7 [40960/60000]\tLoss: 0.073573\n",
            "Train Epoch: 7 [41600/60000]\tLoss: 0.068413\n",
            "Train Epoch: 7 [42240/60000]\tLoss: 0.112518\n",
            "Train Epoch: 7 [42880/60000]\tLoss: 0.038731\n",
            "Train Epoch: 7 [43520/60000]\tLoss: 0.016423\n",
            "Train Epoch: 7 [44160/60000]\tLoss: 0.226661\n",
            "Train Epoch: 7 [44800/60000]\tLoss: 0.103826\n",
            "Train Epoch: 7 [45440/60000]\tLoss: 0.026415\n",
            "Train Epoch: 7 [46080/60000]\tLoss: 0.265428\n",
            "Train Epoch: 7 [46720/60000]\tLoss: 0.025275\n",
            "Train Epoch: 7 [47360/60000]\tLoss: 0.119708\n",
            "Train Epoch: 7 [48000/60000]\tLoss: 0.224972\n",
            "Train Epoch: 7 [48640/60000]\tLoss: 0.062796\n",
            "Train Epoch: 7 [49280/60000]\tLoss: 0.057328\n",
            "Train Epoch: 7 [49920/60000]\tLoss: 0.067579\n",
            "Train Epoch: 7 [50560/60000]\tLoss: 0.099374\n",
            "Train Epoch: 7 [51200/60000]\tLoss: 0.049899\n",
            "Train Epoch: 7 [51840/60000]\tLoss: 0.233429\n",
            "Train Epoch: 7 [52480/60000]\tLoss: 0.046651\n",
            "Train Epoch: 7 [53120/60000]\tLoss: 0.036982\n",
            "Train Epoch: 7 [53760/60000]\tLoss: 0.306643\n",
            "Train Epoch: 7 [54400/60000]\tLoss: 0.281046\n",
            "Train Epoch: 7 [55040/60000]\tLoss: 0.157347\n",
            "Train Epoch: 7 [55680/60000]\tLoss: 0.011627\n",
            "Train Epoch: 7 [56320/60000]\tLoss: 0.111988\n",
            "Train Epoch: 7 [56960/60000]\tLoss: 0.107465\n",
            "Train Epoch: 7 [57600/60000]\tLoss: 0.215637\n",
            "Train Epoch: 7 [58240/60000]\tLoss: 0.046017\n",
            "Train Epoch: 7 [58880/60000]\tLoss: 0.113220\n",
            "Train Epoch: 7 [59520/60000]\tLoss: 0.135230\n",
            "Train Epoch: 8 [0/60000]\tLoss: 0.073489\n",
            "Train Epoch: 8 [640/60000]\tLoss: 0.078472\n",
            "Train Epoch: 8 [1280/60000]\tLoss: 0.048122\n",
            "Train Epoch: 8 [1920/60000]\tLoss: 0.045955\n",
            "Train Epoch: 8 [2560/60000]\tLoss: 0.194204\n",
            "Train Epoch: 8 [3200/60000]\tLoss: 0.036827\n",
            "Train Epoch: 8 [3840/60000]\tLoss: 0.072452\n",
            "Train Epoch: 8 [4480/60000]\tLoss: 0.187019\n",
            "Train Epoch: 8 [5120/60000]\tLoss: 0.081025\n",
            "Train Epoch: 8 [5760/60000]\tLoss: 0.251730\n",
            "Train Epoch: 8 [6400/60000]\tLoss: 0.100742\n",
            "Train Epoch: 8 [7040/60000]\tLoss: 0.021075\n",
            "Train Epoch: 8 [7680/60000]\tLoss: 0.153410\n",
            "Train Epoch: 8 [8320/60000]\tLoss: 0.042218\n",
            "Train Epoch: 8 [8960/60000]\tLoss: 0.070555\n",
            "Train Epoch: 8 [9600/60000]\tLoss: 0.126089\n",
            "Train Epoch: 8 [10240/60000]\tLoss: 0.113231\n",
            "Train Epoch: 8 [10880/60000]\tLoss: 0.066623\n",
            "Train Epoch: 8 [11520/60000]\tLoss: 0.065427\n",
            "Train Epoch: 8 [12160/60000]\tLoss: 0.098321\n",
            "Train Epoch: 8 [12800/60000]\tLoss: 0.161002\n",
            "Train Epoch: 8 [13440/60000]\tLoss: 0.059516\n",
            "Train Epoch: 8 [14080/60000]\tLoss: 0.086824\n",
            "Train Epoch: 8 [14720/60000]\tLoss: 0.087671\n",
            "Train Epoch: 8 [15360/60000]\tLoss: 0.027775\n",
            "Train Epoch: 8 [16000/60000]\tLoss: 0.178202\n",
            "Train Epoch: 8 [16640/60000]\tLoss: 0.074380\n",
            "Train Epoch: 8 [17280/60000]\tLoss: 0.012359\n",
            "Train Epoch: 8 [17920/60000]\tLoss: 0.043038\n",
            "Train Epoch: 8 [18560/60000]\tLoss: 0.089857\n",
            "Train Epoch: 8 [19200/60000]\tLoss: 0.110559\n",
            "Train Epoch: 8 [19840/60000]\tLoss: 0.042502\n",
            "Train Epoch: 8 [20480/60000]\tLoss: 0.080014\n",
            "Train Epoch: 8 [21120/60000]\tLoss: 0.073210\n",
            "Train Epoch: 8 [21760/60000]\tLoss: 0.020552\n",
            "Train Epoch: 8 [22400/60000]\tLoss: 0.039277\n",
            "Train Epoch: 8 [23040/60000]\tLoss: 0.057955\n",
            "Train Epoch: 8 [23680/60000]\tLoss: 0.045297\n",
            "Train Epoch: 8 [24320/60000]\tLoss: 0.187237\n",
            "Train Epoch: 8 [24960/60000]\tLoss: 0.110578\n",
            "Train Epoch: 8 [25600/60000]\tLoss: 0.045316\n",
            "Train Epoch: 8 [26240/60000]\tLoss: 0.025140\n",
            "Train Epoch: 8 [26880/60000]\tLoss: 0.046074\n",
            "Train Epoch: 8 [27520/60000]\tLoss: 0.047084\n",
            "Train Epoch: 8 [28160/60000]\tLoss: 0.067052\n",
            "Train Epoch: 8 [28800/60000]\tLoss: 0.140082\n",
            "Train Epoch: 8 [29440/60000]\tLoss: 0.161937\n",
            "Train Epoch: 8 [30080/60000]\tLoss: 0.072323\n",
            "Train Epoch: 8 [30720/60000]\tLoss: 0.148970\n",
            "Train Epoch: 8 [31360/60000]\tLoss: 0.034908\n",
            "Train Epoch: 8 [32000/60000]\tLoss: 0.053815\n",
            "Train Epoch: 8 [32640/60000]\tLoss: 0.074974\n",
            "Train Epoch: 8 [33280/60000]\tLoss: 0.047456\n",
            "Train Epoch: 8 [33920/60000]\tLoss: 0.091562\n",
            "Train Epoch: 8 [34560/60000]\tLoss: 0.048926\n",
            "Train Epoch: 8 [35200/60000]\tLoss: 0.113745\n",
            "Train Epoch: 8 [35840/60000]\tLoss: 0.123761\n",
            "Train Epoch: 8 [36480/60000]\tLoss: 0.148349\n",
            "Train Epoch: 8 [37120/60000]\tLoss: 0.137343\n",
            "Train Epoch: 8 [37760/60000]\tLoss: 0.058821\n",
            "Train Epoch: 8 [38400/60000]\tLoss: 0.085668\n",
            "Train Epoch: 8 [39040/60000]\tLoss: 0.206685\n",
            "Train Epoch: 8 [39680/60000]\tLoss: 0.062327\n",
            "Train Epoch: 8 [40320/60000]\tLoss: 0.183776\n",
            "Train Epoch: 8 [40960/60000]\tLoss: 0.096509\n",
            "Train Epoch: 8 [41600/60000]\tLoss: 0.016693\n",
            "Train Epoch: 8 [42240/60000]\tLoss: 0.029071\n",
            "Train Epoch: 8 [42880/60000]\tLoss: 0.174208\n",
            "Train Epoch: 8 [43520/60000]\tLoss: 0.130686\n",
            "Train Epoch: 8 [44160/60000]\tLoss: 0.065771\n",
            "Train Epoch: 8 [44800/60000]\tLoss: 0.257958\n",
            "Train Epoch: 8 [45440/60000]\tLoss: 0.098593\n",
            "Train Epoch: 8 [46080/60000]\tLoss: 0.039912\n",
            "Train Epoch: 8 [46720/60000]\tLoss: 0.168675\n",
            "Train Epoch: 8 [47360/60000]\tLoss: 0.133455\n",
            "Train Epoch: 8 [48000/60000]\tLoss: 0.058446\n",
            "Train Epoch: 8 [48640/60000]\tLoss: 0.050084\n",
            "Train Epoch: 8 [49280/60000]\tLoss: 0.049968\n",
            "Train Epoch: 8 [49920/60000]\tLoss: 0.122378\n",
            "Train Epoch: 8 [50560/60000]\tLoss: 0.033148\n",
            "Train Epoch: 8 [51200/60000]\tLoss: 0.154212\n",
            "Train Epoch: 8 [51840/60000]\tLoss: 0.032363\n",
            "Train Epoch: 8 [52480/60000]\tLoss: 0.022615\n",
            "Train Epoch: 8 [53120/60000]\tLoss: 0.028040\n",
            "Train Epoch: 8 [53760/60000]\tLoss: 0.190179\n",
            "Train Epoch: 8 [54400/60000]\tLoss: 0.066844\n",
            "Train Epoch: 8 [55040/60000]\tLoss: 0.124569\n",
            "Train Epoch: 8 [55680/60000]\tLoss: 0.020071\n",
            "Train Epoch: 8 [56320/60000]\tLoss: 0.081703\n",
            "Train Epoch: 8 [56960/60000]\tLoss: 0.067311\n",
            "Train Epoch: 8 [57600/60000]\tLoss: 0.082846\n",
            "Train Epoch: 8 [58240/60000]\tLoss: 0.055944\n",
            "Train Epoch: 8 [58880/60000]\tLoss: 0.228476\n",
            "Train Epoch: 8 [59520/60000]\tLoss: 0.187505\n",
            "Train Epoch: 9 [0/60000]\tLoss: 0.066081\n",
            "Train Epoch: 9 [640/60000]\tLoss: 0.085007\n",
            "Train Epoch: 9 [1280/60000]\tLoss: 0.136099\n",
            "Train Epoch: 9 [1920/60000]\tLoss: 0.052708\n",
            "Train Epoch: 9 [2560/60000]\tLoss: 0.143606\n",
            "Train Epoch: 9 [3200/60000]\tLoss: 0.097273\n",
            "Train Epoch: 9 [3840/60000]\tLoss: 0.055872\n",
            "Train Epoch: 9 [4480/60000]\tLoss: 0.133665\n",
            "Train Epoch: 9 [5120/60000]\tLoss: 0.022911\n",
            "Train Epoch: 9 [5760/60000]\tLoss: 0.096760\n",
            "Train Epoch: 9 [6400/60000]\tLoss: 0.053163\n",
            "Train Epoch: 9 [7040/60000]\tLoss: 0.064642\n",
            "Train Epoch: 9 [7680/60000]\tLoss: 0.012722\n",
            "Train Epoch: 9 [8320/60000]\tLoss: 0.076404\n",
            "Train Epoch: 9 [8960/60000]\tLoss: 0.065439\n",
            "Train Epoch: 9 [9600/60000]\tLoss: 0.068729\n",
            "Train Epoch: 9 [10240/60000]\tLoss: 0.133398\n",
            "Train Epoch: 9 [10880/60000]\tLoss: 0.103231\n",
            "Train Epoch: 9 [11520/60000]\tLoss: 0.046080\n",
            "Train Epoch: 9 [12160/60000]\tLoss: 0.064027\n",
            "Train Epoch: 9 [12800/60000]\tLoss: 0.096475\n",
            "Train Epoch: 9 [13440/60000]\tLoss: 0.021852\n",
            "Train Epoch: 9 [14080/60000]\tLoss: 0.171653\n",
            "Train Epoch: 9 [14720/60000]\tLoss: 0.024855\n",
            "Train Epoch: 9 [15360/60000]\tLoss: 0.059011\n",
            "Train Epoch: 9 [16000/60000]\tLoss: 0.043277\n",
            "Train Epoch: 9 [16640/60000]\tLoss: 0.081688\n",
            "Train Epoch: 9 [17280/60000]\tLoss: 0.013729\n",
            "Train Epoch: 9 [17920/60000]\tLoss: 0.023994\n",
            "Train Epoch: 9 [18560/60000]\tLoss: 0.036212\n",
            "Train Epoch: 9 [19200/60000]\tLoss: 0.158432\n",
            "Train Epoch: 9 [19840/60000]\tLoss: 0.026251\n",
            "Train Epoch: 9 [20480/60000]\tLoss: 0.022002\n",
            "Train Epoch: 9 [21120/60000]\tLoss: 0.181221\n",
            "Train Epoch: 9 [21760/60000]\tLoss: 0.045946\n",
            "Train Epoch: 9 [22400/60000]\tLoss: 0.294416\n",
            "Train Epoch: 9 [23040/60000]\tLoss: 0.033168\n",
            "Train Epoch: 9 [23680/60000]\tLoss: 0.122249\n",
            "Train Epoch: 9 [24320/60000]\tLoss: 0.037687\n",
            "Train Epoch: 9 [24960/60000]\tLoss: 0.012073\n",
            "Train Epoch: 9 [25600/60000]\tLoss: 0.048409\n",
            "Train Epoch: 9 [26240/60000]\tLoss: 0.049667\n",
            "Train Epoch: 9 [26880/60000]\tLoss: 0.168442\n",
            "Train Epoch: 9 [27520/60000]\tLoss: 0.183149\n",
            "Train Epoch: 9 [28160/60000]\tLoss: 0.032988\n",
            "Train Epoch: 9 [28800/60000]\tLoss: 0.048068\n",
            "Train Epoch: 9 [29440/60000]\tLoss: 0.101830\n",
            "Train Epoch: 9 [30080/60000]\tLoss: 0.055368\n",
            "Train Epoch: 9 [30720/60000]\tLoss: 0.072171\n",
            "Train Epoch: 9 [31360/60000]\tLoss: 0.087831\n",
            "Train Epoch: 9 [32000/60000]\tLoss: 0.009237\n",
            "Train Epoch: 9 [32640/60000]\tLoss: 0.026868\n",
            "Train Epoch: 9 [33280/60000]\tLoss: 0.024126\n",
            "Train Epoch: 9 [33920/60000]\tLoss: 0.039788\n",
            "Train Epoch: 9 [34560/60000]\tLoss: 0.065224\n",
            "Train Epoch: 9 [35200/60000]\tLoss: 0.059466\n",
            "Train Epoch: 9 [35840/60000]\tLoss: 0.033826\n",
            "Train Epoch: 9 [36480/60000]\tLoss: 0.023063\n",
            "Train Epoch: 9 [37120/60000]\tLoss: 0.047897\n",
            "Train Epoch: 9 [37760/60000]\tLoss: 0.051284\n",
            "Train Epoch: 9 [38400/60000]\tLoss: 0.084325\n",
            "Train Epoch: 9 [39040/60000]\tLoss: 0.055581\n",
            "Train Epoch: 9 [39680/60000]\tLoss: 0.058090\n",
            "Train Epoch: 9 [40320/60000]\tLoss: 0.007793\n",
            "Train Epoch: 9 [40960/60000]\tLoss: 0.128829\n",
            "Train Epoch: 9 [41600/60000]\tLoss: 0.190855\n",
            "Train Epoch: 9 [42240/60000]\tLoss: 0.141502\n",
            "Train Epoch: 9 [42880/60000]\tLoss: 0.096836\n",
            "Train Epoch: 9 [43520/60000]\tLoss: 0.020715\n",
            "Train Epoch: 9 [44160/60000]\tLoss: 0.077092\n",
            "Train Epoch: 9 [44800/60000]\tLoss: 0.113849\n",
            "Train Epoch: 9 [45440/60000]\tLoss: 0.030254\n",
            "Train Epoch: 9 [46080/60000]\tLoss: 0.085612\n",
            "Train Epoch: 9 [46720/60000]\tLoss: 0.028737\n",
            "Train Epoch: 9 [47360/60000]\tLoss: 0.181194\n",
            "Train Epoch: 9 [48000/60000]\tLoss: 0.047965\n",
            "Train Epoch: 9 [48640/60000]\tLoss: 0.030194\n",
            "Train Epoch: 9 [49280/60000]\tLoss: 0.020982\n",
            "Train Epoch: 9 [49920/60000]\tLoss: 0.088360\n",
            "Train Epoch: 9 [50560/60000]\tLoss: 0.098972\n",
            "Train Epoch: 9 [51200/60000]\tLoss: 0.059227\n",
            "Train Epoch: 9 [51840/60000]\tLoss: 0.126705\n",
            "Train Epoch: 9 [52480/60000]\tLoss: 0.049024\n",
            "Train Epoch: 9 [53120/60000]\tLoss: 0.047720\n",
            "Train Epoch: 9 [53760/60000]\tLoss: 0.079688\n",
            "Train Epoch: 9 [54400/60000]\tLoss: 0.095501\n",
            "Train Epoch: 9 [55040/60000]\tLoss: 0.114005\n",
            "Train Epoch: 9 [55680/60000]\tLoss: 0.145581\n",
            "Train Epoch: 9 [56320/60000]\tLoss: 0.080813\n",
            "Train Epoch: 9 [56960/60000]\tLoss: 0.082731\n",
            "Train Epoch: 9 [57600/60000]\tLoss: 0.075793\n",
            "Train Epoch: 9 [58240/60000]\tLoss: 0.080567\n",
            "Train Epoch: 9 [58880/60000]\tLoss: 0.169292\n",
            "Train Epoch: 9 [59520/60000]\tLoss: 0.126713\n",
            "Train Epoch: 10 [0/60000]\tLoss: 0.173119\n",
            "Train Epoch: 10 [640/60000]\tLoss: 0.046679\n",
            "Train Epoch: 10 [1280/60000]\tLoss: 0.039629\n",
            "Train Epoch: 10 [1920/60000]\tLoss: 0.014297\n",
            "Train Epoch: 10 [2560/60000]\tLoss: 0.027127\n",
            "Train Epoch: 10 [3200/60000]\tLoss: 0.122897\n",
            "Train Epoch: 10 [3840/60000]\tLoss: 0.131117\n",
            "Train Epoch: 10 [4480/60000]\tLoss: 0.044664\n",
            "Train Epoch: 10 [5120/60000]\tLoss: 0.031862\n",
            "Train Epoch: 10 [5760/60000]\tLoss: 0.094973\n",
            "Train Epoch: 10 [6400/60000]\tLoss: 0.153336\n",
            "Train Epoch: 10 [7040/60000]\tLoss: 0.039455\n",
            "Train Epoch: 10 [7680/60000]\tLoss: 0.030456\n",
            "Train Epoch: 10 [8320/60000]\tLoss: 0.020604\n",
            "Train Epoch: 10 [8960/60000]\tLoss: 0.057149\n",
            "Train Epoch: 10 [9600/60000]\tLoss: 0.025048\n",
            "Train Epoch: 10 [10240/60000]\tLoss: 0.090817\n",
            "Train Epoch: 10 [10880/60000]\tLoss: 0.105639\n",
            "Train Epoch: 10 [11520/60000]\tLoss: 0.181958\n",
            "Train Epoch: 10 [12160/60000]\tLoss: 0.039826\n",
            "Train Epoch: 10 [12800/60000]\tLoss: 0.197216\n",
            "Train Epoch: 10 [13440/60000]\tLoss: 0.022562\n",
            "Train Epoch: 10 [14080/60000]\tLoss: 0.070542\n",
            "Train Epoch: 10 [14720/60000]\tLoss: 0.042104\n",
            "Train Epoch: 10 [15360/60000]\tLoss: 0.072172\n",
            "Train Epoch: 10 [16000/60000]\tLoss: 0.022394\n",
            "Train Epoch: 10 [16640/60000]\tLoss: 0.240360\n",
            "Train Epoch: 10 [17280/60000]\tLoss: 0.106799\n",
            "Train Epoch: 10 [17920/60000]\tLoss: 0.016155\n",
            "Train Epoch: 10 [18560/60000]\tLoss: 0.048331\n",
            "Train Epoch: 10 [19200/60000]\tLoss: 0.053063\n",
            "Train Epoch: 10 [19840/60000]\tLoss: 0.101828\n",
            "Train Epoch: 10 [20480/60000]\tLoss: 0.027956\n",
            "Train Epoch: 10 [21120/60000]\tLoss: 0.132090\n",
            "Train Epoch: 10 [21760/60000]\tLoss: 0.095148\n",
            "Train Epoch: 10 [22400/60000]\tLoss: 0.081912\n",
            "Train Epoch: 10 [23040/60000]\tLoss: 0.025109\n",
            "Train Epoch: 10 [23680/60000]\tLoss: 0.036981\n",
            "Train Epoch: 10 [24320/60000]\tLoss: 0.137848\n",
            "Train Epoch: 10 [24960/60000]\tLoss: 0.036363\n",
            "Train Epoch: 10 [25600/60000]\tLoss: 0.130581\n",
            "Train Epoch: 10 [26240/60000]\tLoss: 0.011571\n",
            "Train Epoch: 10 [26880/60000]\tLoss: 0.155715\n",
            "Train Epoch: 10 [27520/60000]\tLoss: 0.115775\n",
            "Train Epoch: 10 [28160/60000]\tLoss: 0.075748\n",
            "Train Epoch: 10 [28800/60000]\tLoss: 0.049825\n",
            "Train Epoch: 10 [29440/60000]\tLoss: 0.076385\n",
            "Train Epoch: 10 [30080/60000]\tLoss: 0.023694\n",
            "Train Epoch: 10 [30720/60000]\tLoss: 0.087288\n",
            "Train Epoch: 10 [31360/60000]\tLoss: 0.207463\n",
            "Train Epoch: 10 [32000/60000]\tLoss: 0.022995\n",
            "Train Epoch: 10 [32640/60000]\tLoss: 0.123367\n",
            "Train Epoch: 10 [33280/60000]\tLoss: 0.104357\n",
            "Train Epoch: 10 [33920/60000]\tLoss: 0.260008\n",
            "Train Epoch: 10 [34560/60000]\tLoss: 0.012562\n",
            "Train Epoch: 10 [35200/60000]\tLoss: 0.014584\n",
            "Train Epoch: 10 [35840/60000]\tLoss: 0.232272\n",
            "Train Epoch: 10 [36480/60000]\tLoss: 0.055927\n",
            "Train Epoch: 10 [37120/60000]\tLoss: 0.062854\n",
            "Train Epoch: 10 [37760/60000]\tLoss: 0.026889\n",
            "Train Epoch: 10 [38400/60000]\tLoss: 0.043602\n",
            "Train Epoch: 10 [39040/60000]\tLoss: 0.044411\n",
            "Train Epoch: 10 [39680/60000]\tLoss: 0.084038\n",
            "Train Epoch: 10 [40320/60000]\tLoss: 0.008054\n",
            "Train Epoch: 10 [40960/60000]\tLoss: 0.064145\n",
            "Train Epoch: 10 [41600/60000]\tLoss: 0.114714\n",
            "Train Epoch: 10 [42240/60000]\tLoss: 0.040042\n",
            "Train Epoch: 10 [42880/60000]\tLoss: 0.127072\n",
            "Train Epoch: 10 [43520/60000]\tLoss: 0.072883\n",
            "Train Epoch: 10 [44160/60000]\tLoss: 0.018051\n",
            "Train Epoch: 10 [44800/60000]\tLoss: 0.061856\n",
            "Train Epoch: 10 [45440/60000]\tLoss: 0.052345\n",
            "Train Epoch: 10 [46080/60000]\tLoss: 0.025639\n",
            "Train Epoch: 10 [46720/60000]\tLoss: 0.079037\n",
            "Train Epoch: 10 [47360/60000]\tLoss: 0.558575\n",
            "Train Epoch: 10 [48000/60000]\tLoss: 0.021959\n",
            "Train Epoch: 10 [48640/60000]\tLoss: 0.183354\n",
            "Train Epoch: 10 [49280/60000]\tLoss: 0.147573\n",
            "Train Epoch: 10 [49920/60000]\tLoss: 0.090488\n",
            "Train Epoch: 10 [50560/60000]\tLoss: 0.141777\n",
            "Train Epoch: 10 [51200/60000]\tLoss: 0.009246\n",
            "Train Epoch: 10 [51840/60000]\tLoss: 0.352812\n",
            "Train Epoch: 10 [52480/60000]\tLoss: 0.036175\n",
            "Train Epoch: 10 [53120/60000]\tLoss: 0.037928\n",
            "Train Epoch: 10 [53760/60000]\tLoss: 0.094357\n",
            "Train Epoch: 10 [54400/60000]\tLoss: 0.073211\n",
            "Train Epoch: 10 [55040/60000]\tLoss: 0.085179\n",
            "Train Epoch: 10 [55680/60000]\tLoss: 0.087428\n",
            "Train Epoch: 10 [56320/60000]\tLoss: 0.086289\n",
            "Train Epoch: 10 [56960/60000]\tLoss: 0.058952\n",
            "Train Epoch: 10 [57600/60000]\tLoss: 0.081244\n",
            "Train Epoch: 10 [58240/60000]\tLoss: 0.161878\n",
            "Train Epoch: 10 [58880/60000]\tLoss: 0.106484\n",
            "Train Epoch: 10 [59520/60000]\tLoss: 0.039767\n",
            "\n",
            "Test set: Avg. loss: 0.0364, Accuracy: 9888/10000 (99%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train and test Model 2\n",
        "\n",
        "# Create network\n",
        "model2 = Net2()\n",
        "# Initialize model weights\n",
        "model2.apply(weights_init)\n",
        "# Define optimizer\n",
        "optimizer = optim.SGD(model2.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "# Get initial performance\n",
        "test(model2)\n",
        "# Train for ten epochs\n",
        "n_epochs = 10\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(epoch, model2)\n",
        "accuracy2 = test(model2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFbCnAUmTwyx"
      },
      "source": [
        "## III. Results\n",
        "\n",
        "Here we train the CNN model and apply it to the test set. There are 10 epochs in training. There is no validation set here, we simply take the model at the end of the training procedure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgAKHjLbqm3S",
        "outputId": "1496d891-5509-49d6-ef91-2dec981234d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1 Accuracy: 92.38%\n",
            "Model 2 Accuracy: 98.88%\n"
          ]
        }
      ],
      "source": [
        "print(f\"Model 1 Accuracy: {round(float(accuracy1.numpy()),2)}%\")\n",
        "print(f\"Model 2 Accuracy: {round(float(accuracy2.numpy()),2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "8hG1l1rSulbg",
        "outputId": "ea1b3445-8b4e-476c-f862-7e04239e6def"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-b8ebf8cbff0b>:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Run network on data we got before and show predictions\n",
        "output = model(example_data)\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(10):\n",
        "  plt.subplot(5,5,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Prediction: {}\".format(\n",
        "    output.data.max(1, keepdim=True)[1][i].item()))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "zUHLA7qru5cQ",
        "outputId": "f01e4474-247b-48b4-ecb3-b8fba05161f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-b4df5afe933b>:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Run network on data we got before and show predictions\n",
        "output = model2(example_data)\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(10):\n",
        "  plt.subplot(5,5,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Prediction: {}\".format(\n",
        "    output.data.max(1, keepdim=True)[1][i].item()))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0w7iym1T2QY"
      },
      "source": [
        "# IV. Conclusion and Discussion\n",
        "\n",
        "# Write something"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmIihlpfT2pk"
      },
      "outputs": [],
      "source": [
        "#The first model (93% accuracy) performs worse than the second model (99% accuracy).\n",
        "#The reason is that the second model contains a deeper architecture, which may have more capacity to learn more infomation from the features.\n",
        "#Besides, the second model uses dropout method, which is a regularization technique that combats overfitting.\n",
        "#Last but not least, maybe the value of parameters of the second model is beneficial for this task, and then the number of the parameters\n",
        "#of the second model is enough to learn the representation of the data.\n",
        "#Hence, the second model has a better performance."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}